{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dOyBkSsMhyOa",
        "outputId": "b6d8132c-e5d4-471c-810f-0e34bc7e7561"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: wget in /usr/local/lib/python3.10/dist-packages (3.2)\n"
          ]
        }
      ],
      "source": [
        "!pip install wget"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Czg9eMpNxunP",
        "outputId": "37297298-e121-4e9c-e271-4900455ee1f3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: thop in /usr/local/lib/python3.10/dist-packages (0.1.1.post2209072238)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from thop) (2.0.0+cu118)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->thop) (4.5.0)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->thop) (1.11.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->thop) (3.1)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->thop) (3.1.2)\n",
            "Requirement already satisfied: triton==2.0.0 in /usr/local/lib/python3.10/dist-packages (from torch->thop) (2.0.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->thop) (3.12.0)\n",
            "Requirement already satisfied: cmake in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->thop) (3.25.2)\n",
            "Requirement already satisfied: lit in /usr/local/lib/python3.10/dist-packages (from triton==2.0.0->torch->thop) (16.0.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->thop) (2.1.2)\n",
            "Requirement already satisfied: mpmath>=0.19 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->thop) (1.3.0)\n"
          ]
        }
      ],
      "source": [
        "! pip install thop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "TQZHKlBbfe35"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import os.path\n",
        "import wget\n",
        "from zipfile import ZipFile\n",
        "import copy\n",
        "import numpy as np\n",
        "from torchvision import datasets, transforms\n",
        "import torch\n",
        "\n",
        "from utils.sampling import cifar_noniid, cifar_iid\n",
        "from utils.options import args_parser\n",
        "from models.Update import LocalUpdate\n",
        "from models.Nets import MobileNetV2\n",
        "from models.Fed import FedAvg\n",
        "from models.Test import test_img\n",
        "from utils.util import setup_seed\n",
        "from utils.util import exp_details\n",
        "from utils.dataset_reader import TinyImageNetDataset\n",
        "from datetime import datetime\n",
        "import torchvision.models as models\n",
        "from torch.utils.tensorboard import SummaryWriter"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class Argments:\n",
        "    def __init__(self):\n",
        "        self.epochs = 10\n",
        "        self.num_users = 5\n",
        "        self.frac = 0.5\n",
        "        self.local_ep = 5\n",
        "        self.local_bs = 32\n",
        "        self.test_bs = 32\n",
        "        self.lr = 0.01\n",
        "        self.momentum = 0.5\n",
        "        self.split = 'user'\n",
        "        \n",
        "        self.model = 'mobilenet'\n",
        "        \n",
        "        self.rebuild = 1\n",
        "        self.struct = 1\n",
        "        self.dataset = 'imagenet'\n",
        "        self.iid = True\n",
        "        self.alpha = 0.5\n",
        "        self.num_classes = 200\n",
        "        self.num_channels = 3\n",
        "        self.gpu = 0\n",
        "        self.stopping_rounds = 10\n",
        "        self.verbose = 1\n",
        "        self.debug = 1\n",
        "        self.seed = 1"
      ],
      "metadata": {
        "id": "IhAUFfZPzPHa"
      },
      "execution_count": 32,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "ocdkdI3Cw_2p",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fcd4c9ba-538d-4162-b936-1dc413efb268"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "---------------Experimental details:-------------\n",
            "\n",
            "\tModel           : mobilenet\n",
            "\ttLearning       : 0.01\n",
            "\tGlobal Rounds   : 10\n",
            "\n",
            "\n",
            "----------------Federated parameters:------------\n",
            "\n",
            "\tIID\n",
            "\tFraction of users  : 0.5\n",
            "\tLocal Batch size   : 32\n",
            "\tLocal Epochs       : 5\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# parse args\n",
        "args = Argments()\n",
        "args.device = torch.device('cuda:{}'.format(args.gpu) if torch.cuda.is_available() and args.gpu != -1 else 'cpu')\n",
        "setup_seed(args.seed)\n",
        "exp_details(args)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "83BsQL0jw_w5"
      },
      "outputs": [],
      "source": [
        "current_time = datetime.now().strftime('%b.%d_%H.%M.%S')\n",
        "TAG = 'exp/fed/{}_{}_{}_C{}_iid{}_{}_user{}_{}'.format(args.dataset, args.model, args.epochs, args.frac, args.iid,\n",
        "                                                        args.alpha, args.num_users, current_time)\n",
        "# TAG = f'alpha_{alpha}/data_distribution'\n",
        "logdir = f'runs/{TAG}' if not args.debug else f'runs2/{TAG}'\n",
        "writer = SummaryWriter(logdir)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "dxS1n0aiw_tz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "431896bd-2873-480a-c743-d071c66b71cb"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Dataset is already downloaded\n",
            "\n"
          ]
        }
      ],
      "source": [
        "if args.dataset == 'imagenet':\n",
        "    TINY_IMAGENET_ROOT = 'data/tiny-imagenet-200/'\n",
        "    if os.path.exists('tiny-imagenet-200.zip') == False:\n",
        "        print('\\nDownload dataset\\n')\n",
        "        url = 'http://cs231n.stanford.edu/tiny-imagenet-200.zip'\n",
        "        tiny_imgdataset = wget.download(url, out = os.getcwd())\n",
        "        with ZipFile('tiny-imagenet-200.zip', 'r') as zip_ref:\n",
        "            zip_ref.extractall('data/')\n",
        "    else:\n",
        "        print('\\nDataset is already downloaded\\n')\n",
        "\n",
        "\n",
        "    dataset_train = datasets.ImageFolder(\n",
        "        os.path.join('data/', 'tiny-imagenet-200', 'train'),\n",
        "        transform=transforms.Compose(\n",
        "            [\n",
        "                transforms.Resize(256),\n",
        "                transforms.CenterCrop(224),\n",
        "                transforms.RandomRotation(20),\n",
        "                transforms.RandomHorizontalFlip(0.5),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "            ]\n",
        "        )\n",
        "    )\n",
        "    dataset_test = TinyImageNetDataset(\n",
        "        img_path=os.path.join('data/', 'tiny-imagenet-200', 'val', 'images'), \n",
        "        gt_path=os.path.join('data/', 'tiny-imagenet-200', 'val', 'val_annotations.txt'),\n",
        "        class_to_idx=dataset_train.class_to_idx.copy(),\n",
        "        transform=transforms.Compose(\n",
        "            [\n",
        "                transforms.RandomResizedCrop(224),\n",
        "                transforms.RandomHorizontalFlip(),\n",
        "                transforms.ToTensor(),\n",
        "                transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
        "            ]\n",
        "        )\n",
        "    )\n",
        "    \n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "wfhpFGJOw_ro",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b7ac034-f43a-4ae4-ef49-d77db2814e40"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "start separate dataset for iid\n",
            "end\n"
          ]
        }
      ],
      "source": [
        "if args.iid:\n",
        "    print('start separate dataset for iid')\n",
        "    dict_users = cifar_iid(dataset_train, args.num_users)\n",
        "    print('end')\n",
        "else:\n",
        "    print('start separate dataset for non-iid')\n",
        "    dict_users, _ = cifar_noniid(dataset_train, args.num_users, args.alpha)\n",
        "    for k, v in dict_users.items():\n",
        "        writer.add_histogram(f'user_{k}/data_distribution',\n",
        "                            np.array(dataset_train.targets)[v],\n",
        "                            bins=np.arange(11))\n",
        "        writer.add_histogram(f'all_user/data_distribution',\n",
        "                            np.array(dataset_train.targets)[v],\n",
        "                            bins=np.arange(11), global_step=k)\n",
        "    print('end')\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "wVMFHo87w_pY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5a3b7717-fa4e-421a-8c9f-fc7f2e757d3f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MobileNetV3(\n",
            "  (features): Sequential(\n",
            "    (0): Conv2dNormActivation(\n",
            "      (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "      (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
            "      (2): Hardswish()\n",
            "    )\n",
            "    (1): InvertedResidual(\n",
            "      (block): Sequential(\n",
            "        (0): Conv2dNormActivation(\n",
            "          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=16, bias=False)\n",
            "          (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
            "          (2): ReLU(inplace=True)\n",
            "        )\n",
            "        (1): SqueezeExcitation(\n",
            "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
            "          (fc1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (fc2): Conv2d(8, 16, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (activation): ReLU()\n",
            "          (scale_activation): Hardsigmoid()\n",
            "        )\n",
            "        (2): Conv2dNormActivation(\n",
            "          (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (2): InvertedResidual(\n",
            "      (block): Sequential(\n",
            "        (0): Conv2dNormActivation(\n",
            "          (0): Conv2d(16, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
            "          (2): ReLU(inplace=True)\n",
            "        )\n",
            "        (1): Conv2dNormActivation(\n",
            "          (0): Conv2d(72, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=72, bias=False)\n",
            "          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
            "          (2): ReLU(inplace=True)\n",
            "        )\n",
            "        (2): Conv2dNormActivation(\n",
            "          (0): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (3): InvertedResidual(\n",
            "      (block): Sequential(\n",
            "        (0): Conv2dNormActivation(\n",
            "          (0): Conv2d(24, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(88, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
            "          (2): ReLU(inplace=True)\n",
            "        )\n",
            "        (1): Conv2dNormActivation(\n",
            "          (0): Conv2d(88, 88, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=88, bias=False)\n",
            "          (1): BatchNorm2d(88, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
            "          (2): ReLU(inplace=True)\n",
            "        )\n",
            "        (2): Conv2dNormActivation(\n",
            "          (0): Conv2d(88, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (4): InvertedResidual(\n",
            "      (block): Sequential(\n",
            "        (0): Conv2dNormActivation(\n",
            "          (0): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
            "          (2): Hardswish()\n",
            "        )\n",
            "        (1): Conv2dNormActivation(\n",
            "          (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96, bias=False)\n",
            "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
            "          (2): Hardswish()\n",
            "        )\n",
            "        (2): SqueezeExcitation(\n",
            "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
            "          (fc1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (fc2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (activation): ReLU()\n",
            "          (scale_activation): Hardsigmoid()\n",
            "        )\n",
            "        (3): Conv2dNormActivation(\n",
            "          (0): Conv2d(96, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (5): InvertedResidual(\n",
            "      (block): Sequential(\n",
            "        (0): Conv2dNormActivation(\n",
            "          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
            "          (2): Hardswish()\n",
            "        )\n",
            "        (1): Conv2dNormActivation(\n",
            "          (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
            "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
            "          (2): Hardswish()\n",
            "        )\n",
            "        (2): SqueezeExcitation(\n",
            "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
            "          (fc1): Conv2d(240, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (fc2): Conv2d(64, 240, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (activation): ReLU()\n",
            "          (scale_activation): Hardsigmoid()\n",
            "        )\n",
            "        (3): Conv2dNormActivation(\n",
            "          (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (6): InvertedResidual(\n",
            "      (block): Sequential(\n",
            "        (0): Conv2dNormActivation(\n",
            "          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
            "          (2): Hardswish()\n",
            "        )\n",
            "        (1): Conv2dNormActivation(\n",
            "          (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
            "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
            "          (2): Hardswish()\n",
            "        )\n",
            "        (2): SqueezeExcitation(\n",
            "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
            "          (fc1): Conv2d(240, 64, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (fc2): Conv2d(64, 240, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (activation): ReLU()\n",
            "          (scale_activation): Hardsigmoid()\n",
            "        )\n",
            "        (3): Conv2dNormActivation(\n",
            "          (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (7): InvertedResidual(\n",
            "      (block): Sequential(\n",
            "        (0): Conv2dNormActivation(\n",
            "          (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
            "          (2): Hardswish()\n",
            "        )\n",
            "        (1): Conv2dNormActivation(\n",
            "          (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
            "          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
            "          (2): Hardswish()\n",
            "        )\n",
            "        (2): SqueezeExcitation(\n",
            "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
            "          (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (activation): ReLU()\n",
            "          (scale_activation): Hardsigmoid()\n",
            "        )\n",
            "        (3): Conv2dNormActivation(\n",
            "          (0): Conv2d(120, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (8): InvertedResidual(\n",
            "      (block): Sequential(\n",
            "        (0): Conv2dNormActivation(\n",
            "          (0): Conv2d(48, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
            "          (2): Hardswish()\n",
            "        )\n",
            "        (1): Conv2dNormActivation(\n",
            "          (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=144, bias=False)\n",
            "          (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
            "          (2): Hardswish()\n",
            "        )\n",
            "        (2): SqueezeExcitation(\n",
            "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
            "          (fc1): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (fc2): Conv2d(40, 144, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (activation): ReLU()\n",
            "          (scale_activation): Hardsigmoid()\n",
            "        )\n",
            "        (3): Conv2dNormActivation(\n",
            "          (0): Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (9): InvertedResidual(\n",
            "      (block): Sequential(\n",
            "        (0): Conv2dNormActivation(\n",
            "          (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(288, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
            "          (2): Hardswish()\n",
            "        )\n",
            "        (1): Conv2dNormActivation(\n",
            "          (0): Conv2d(288, 288, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=288, bias=False)\n",
            "          (1): BatchNorm2d(288, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
            "          (2): Hardswish()\n",
            "        )\n",
            "        (2): SqueezeExcitation(\n",
            "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
            "          (fc1): Conv2d(288, 72, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (fc2): Conv2d(72, 288, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (activation): ReLU()\n",
            "          (scale_activation): Hardsigmoid()\n",
            "        )\n",
            "        (3): Conv2dNormActivation(\n",
            "          (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (10): InvertedResidual(\n",
            "      (block): Sequential(\n",
            "        (0): Conv2dNormActivation(\n",
            "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
            "          (2): Hardswish()\n",
            "        )\n",
            "        (1): Conv2dNormActivation(\n",
            "          (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n",
            "          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
            "          (2): Hardswish()\n",
            "        )\n",
            "        (2): SqueezeExcitation(\n",
            "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
            "          (fc1): Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (fc2): Conv2d(144, 576, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (activation): ReLU()\n",
            "          (scale_activation): Hardsigmoid()\n",
            "        )\n",
            "        (3): Conv2dNormActivation(\n",
            "          (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (11): InvertedResidual(\n",
            "      (block): Sequential(\n",
            "        (0): Conv2dNormActivation(\n",
            "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
            "          (2): Hardswish()\n",
            "        )\n",
            "        (1): Conv2dNormActivation(\n",
            "          (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n",
            "          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
            "          (2): Hardswish()\n",
            "        )\n",
            "        (2): SqueezeExcitation(\n",
            "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
            "          (fc1): Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (fc2): Conv2d(144, 576, kernel_size=(1, 1), stride=(1, 1))\n",
            "          (activation): ReLU()\n",
            "          (scale_activation): Hardsigmoid()\n",
            "        )\n",
            "        (3): Conv2dNormActivation(\n",
            "          (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (12): Conv2dNormActivation(\n",
            "      (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
            "      (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
            "      (2): Hardswish()\n",
            "    )\n",
            "  )\n",
            "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
            "  (classifier): Sequential(\n",
            "    (0): Linear(in_features=576, out_features=1024, bias=True)\n",
            "    (1): Hardswish()\n",
            "    (2): Dropout(p=0.2, inplace=True)\n",
            "    (3): Linear(in_features=1024, out_features=1000, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=MobileNet_V3_Small_Weights.IMAGENET1K_V1`. You can also use `weights=MobileNet_V3_Small_Weights.DEFAULT` to get the most up-to-date weights.\n",
            "  warnings.warn(msg)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "MobileNetV3(\n",
              "  (features): Sequential(\n",
              "    (0): Conv2dNormActivation(\n",
              "      (0): Conv2d(3, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "      (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "      (2): Hardswish()\n",
              "    )\n",
              "    (1): InvertedResidual(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(16, 16, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=16, bias=False)\n",
              "          (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): ReLU(inplace=True)\n",
              "        )\n",
              "        (1): SqueezeExcitation(\n",
              "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "          (fc1): Conv2d(16, 8, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (fc2): Conv2d(8, 16, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (activation): ReLU()\n",
              "          (scale_activation): Hardsigmoid()\n",
              "        )\n",
              "        (2): Conv2dNormActivation(\n",
              "          (0): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (2): InvertedResidual(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(16, 72, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): ReLU(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(72, 72, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), groups=72, bias=False)\n",
              "          (1): BatchNorm2d(72, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Conv2dNormActivation(\n",
              "          (0): Conv2d(72, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (3): InvertedResidual(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(24, 88, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(88, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): ReLU(inplace=True)\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(88, 88, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=88, bias=False)\n",
              "          (1): BatchNorm2d(88, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): ReLU(inplace=True)\n",
              "        )\n",
              "        (2): Conv2dNormActivation(\n",
              "          (0): Conv2d(88, 24, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(24, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (4): InvertedResidual(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): Hardswish()\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(96, 96, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=96, bias=False)\n",
              "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): Hardswish()\n",
              "        )\n",
              "        (2): SqueezeExcitation(\n",
              "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "          (fc1): Conv2d(96, 24, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (fc2): Conv2d(24, 96, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (activation): ReLU()\n",
              "          (scale_activation): Hardsigmoid()\n",
              "        )\n",
              "        (3): Conv2dNormActivation(\n",
              "          (0): Conv2d(96, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (5): InvertedResidual(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): Hardswish()\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
              "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): Hardswish()\n",
              "        )\n",
              "        (2): SqueezeExcitation(\n",
              "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "          (fc1): Conv2d(240, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (fc2): Conv2d(64, 240, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (activation): ReLU()\n",
              "          (scale_activation): Hardsigmoid()\n",
              "        )\n",
              "        (3): Conv2dNormActivation(\n",
              "          (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (6): InvertedResidual(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(40, 240, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): Hardswish()\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(240, 240, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=240, bias=False)\n",
              "          (1): BatchNorm2d(240, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): Hardswish()\n",
              "        )\n",
              "        (2): SqueezeExcitation(\n",
              "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "          (fc1): Conv2d(240, 64, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (fc2): Conv2d(64, 240, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (activation): ReLU()\n",
              "          (scale_activation): Hardsigmoid()\n",
              "        )\n",
              "        (3): Conv2dNormActivation(\n",
              "          (0): Conv2d(240, 40, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(40, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (7): InvertedResidual(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(40, 120, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): Hardswish()\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(120, 120, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=120, bias=False)\n",
              "          (1): BatchNorm2d(120, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): Hardswish()\n",
              "        )\n",
              "        (2): SqueezeExcitation(\n",
              "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "          (fc1): Conv2d(120, 32, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (fc2): Conv2d(32, 120, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (activation): ReLU()\n",
              "          (scale_activation): Hardsigmoid()\n",
              "        )\n",
              "        (3): Conv2dNormActivation(\n",
              "          (0): Conv2d(120, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (8): InvertedResidual(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(48, 144, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): Hardswish()\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(144, 144, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=144, bias=False)\n",
              "          (1): BatchNorm2d(144, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): Hardswish()\n",
              "        )\n",
              "        (2): SqueezeExcitation(\n",
              "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "          (fc1): Conv2d(144, 40, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (fc2): Conv2d(40, 144, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (activation): ReLU()\n",
              "          (scale_activation): Hardsigmoid()\n",
              "        )\n",
              "        (3): Conv2dNormActivation(\n",
              "          (0): Conv2d(144, 48, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(48, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (9): InvertedResidual(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(48, 288, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(288, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): Hardswish()\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(288, 288, kernel_size=(5, 5), stride=(2, 2), padding=(2, 2), groups=288, bias=False)\n",
              "          (1): BatchNorm2d(288, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): Hardswish()\n",
              "        )\n",
              "        (2): SqueezeExcitation(\n",
              "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "          (fc1): Conv2d(288, 72, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (fc2): Conv2d(72, 288, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (activation): ReLU()\n",
              "          (scale_activation): Hardsigmoid()\n",
              "        )\n",
              "        (3): Conv2dNormActivation(\n",
              "          (0): Conv2d(288, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (10): InvertedResidual(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): Hardswish()\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n",
              "          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): Hardswish()\n",
              "        )\n",
              "        (2): SqueezeExcitation(\n",
              "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "          (fc1): Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (fc2): Conv2d(144, 576, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (activation): ReLU()\n",
              "          (scale_activation): Hardsigmoid()\n",
              "        )\n",
              "        (3): Conv2dNormActivation(\n",
              "          (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (11): InvertedResidual(\n",
              "      (block): Sequential(\n",
              "        (0): Conv2dNormActivation(\n",
              "          (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): Hardswish()\n",
              "        )\n",
              "        (1): Conv2dNormActivation(\n",
              "          (0): Conv2d(576, 576, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=576, bias=False)\n",
              "          (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "          (2): Hardswish()\n",
              "        )\n",
              "        (2): SqueezeExcitation(\n",
              "          (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "          (fc1): Conv2d(576, 144, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (fc2): Conv2d(144, 576, kernel_size=(1, 1), stride=(1, 1))\n",
              "          (activation): ReLU()\n",
              "          (scale_activation): Hardsigmoid()\n",
              "        )\n",
              "        (3): Conv2dNormActivation(\n",
              "          (0): Conv2d(576, 96, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(96, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (12): Conv2dNormActivation(\n",
              "      (0): Conv2d(96, 576, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "      (1): BatchNorm2d(576, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
              "      (2): Hardswish()\n",
              "    )\n",
              "  )\n",
              "  (avgpool): AdaptiveAvgPool2d(output_size=1)\n",
              "  (classifier): Sequential(\n",
              "    (0): Linear(in_features=576, out_features=1024, bias=True)\n",
              "    (1): Hardswish()\n",
              "    (2): Dropout(p=0.2, inplace=True)\n",
              "    (3): Linear(in_features=1024, out_features=1000, bias=True)\n",
              "  )\n",
              ")"
            ]
          },
          "metadata": {},
          "execution_count": 37
        }
      ],
      "source": [
        "# build model\n",
        "if args.model == 'mobilenet' and args.dataset == 'imagenet':\n",
        "    # net_glob = MobileNetV2().to(args.device)\n",
        "    net_glob = models.mobilenet_v3_small(pretrained=True).to(args.device)\n",
        "else:\n",
        "    exit('Error: unrecognized model')\n",
        "print(net_glob)\n",
        "net_glob.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "48AnA7DDw_l4"
      },
      "outputs": [],
      "source": [
        "# copy weights\n",
        "w_glob = net_glob.state_dict()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# training\n",
        "loss_train = []\n",
        "cv_loss, cv_acc = [], []\n",
        "val_loss_pre, counter = 0, 0\n",
        "net_best = None\n",
        "best_loss = None\n",
        "val_acc_list, net_list = [], []\n",
        "test_best_acc = 0.0"
      ],
      "metadata": {
        "id": "RR-pSmDR0VFv"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "jhEEDozOw_jg",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "outputId": "74018722-1f80-45d3-a723-8cdce35513ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Global epoch 0\n",
            "\n",
            "\n",
            "client 4\n",
            "\n",
            "Update Epoch: 0 [0/20000 (0%)]\tLoss: 9.824431\n",
            "Update Epoch: 0 [320/20000 (2%)]\tLoss: 7.755905\n",
            "Update Epoch: 0 [640/20000 (3%)]\tLoss: 7.338758\n",
            "Update Epoch: 0 [960/20000 (5%)]\tLoss: 6.574852\n",
            "Update Epoch: 0 [1280/20000 (6%)]\tLoss: 6.347293\n",
            "Update Epoch: 0 [1600/20000 (8%)]\tLoss: 6.323892\n",
            "Update Epoch: 0 [1920/20000 (10%)]\tLoss: 5.944541\n",
            "Update Epoch: 0 [2240/20000 (11%)]\tLoss: 5.772562\n",
            "Update Epoch: 0 [2560/20000 (13%)]\tLoss: 6.269417\n",
            "Update Epoch: 0 [2880/20000 (14%)]\tLoss: 5.795028\n",
            "Update Epoch: 0 [3200/20000 (16%)]\tLoss: 5.612286\n",
            "Update Epoch: 0 [3520/20000 (18%)]\tLoss: 5.409889\n",
            "Update Epoch: 0 [3840/20000 (19%)]\tLoss: 6.174156\n",
            "Update Epoch: 0 [4160/20000 (21%)]\tLoss: 5.764199\n",
            "Update Epoch: 0 [4480/20000 (22%)]\tLoss: 5.522725\n",
            "Update Epoch: 0 [4800/20000 (24%)]\tLoss: 5.961246\n",
            "Update Epoch: 0 [5120/20000 (26%)]\tLoss: 5.786539\n",
            "Update Epoch: 0 [5440/20000 (27%)]\tLoss: 5.491814\n",
            "Update Epoch: 0 [5760/20000 (29%)]\tLoss: 5.656126\n",
            "Update Epoch: 0 [6080/20000 (30%)]\tLoss: 5.730626\n",
            "Update Epoch: 0 [6400/20000 (32%)]\tLoss: 5.608405\n",
            "Update Epoch: 0 [6720/20000 (34%)]\tLoss: 5.496814\n",
            "Update Epoch: 0 [7040/20000 (35%)]\tLoss: 5.386728\n",
            "Update Epoch: 0 [7360/20000 (37%)]\tLoss: 5.459048\n",
            "Update Epoch: 0 [7680/20000 (38%)]\tLoss: 5.456720\n",
            "Update Epoch: 0 [8000/20000 (40%)]\tLoss: 5.162797\n",
            "Update Epoch: 0 [8320/20000 (42%)]\tLoss: 5.653903\n",
            "Update Epoch: 0 [8640/20000 (43%)]\tLoss: 5.291355\n",
            "Update Epoch: 0 [8960/20000 (45%)]\tLoss: 5.636431\n",
            "Update Epoch: 0 [9280/20000 (46%)]\tLoss: 5.202730\n",
            "Update Epoch: 0 [9600/20000 (48%)]\tLoss: 4.999900\n",
            "Update Epoch: 0 [9920/20000 (50%)]\tLoss: 5.022537\n",
            "Update Epoch: 0 [10240/20000 (51%)]\tLoss: 5.300675\n",
            "Update Epoch: 0 [10560/20000 (53%)]\tLoss: 5.055327\n",
            "Update Epoch: 0 [10880/20000 (54%)]\tLoss: 5.313655\n",
            "Update Epoch: 0 [11200/20000 (56%)]\tLoss: 4.855955\n",
            "Update Epoch: 0 [11520/20000 (58%)]\tLoss: 5.528480\n",
            "Update Epoch: 0 [11840/20000 (59%)]\tLoss: 5.010275\n",
            "Update Epoch: 0 [12160/20000 (61%)]\tLoss: 5.034765\n",
            "Update Epoch: 0 [12480/20000 (62%)]\tLoss: 5.309821\n",
            "Update Epoch: 0 [12800/20000 (64%)]\tLoss: 5.189917\n",
            "Update Epoch: 0 [13120/20000 (66%)]\tLoss: 5.032371\n",
            "Update Epoch: 0 [13440/20000 (67%)]\tLoss: 4.793319\n",
            "Update Epoch: 0 [13760/20000 (69%)]\tLoss: 4.955414\n",
            "Update Epoch: 0 [14080/20000 (70%)]\tLoss: 4.692566\n",
            "Update Epoch: 0 [14400/20000 (72%)]\tLoss: 4.982254\n",
            "Update Epoch: 0 [14720/20000 (74%)]\tLoss: 4.973147\n",
            "Update Epoch: 0 [15040/20000 (75%)]\tLoss: 5.151584\n",
            "Update Epoch: 0 [15360/20000 (77%)]\tLoss: 4.620822\n",
            "Update Epoch: 0 [15680/20000 (78%)]\tLoss: 4.736853\n",
            "Update Epoch: 0 [16000/20000 (80%)]\tLoss: 4.920201\n",
            "Update Epoch: 0 [16320/20000 (82%)]\tLoss: 4.829648\n",
            "Update Epoch: 0 [16640/20000 (83%)]\tLoss: 5.086150\n",
            "Update Epoch: 0 [16960/20000 (85%)]\tLoss: 5.301736\n",
            "Update Epoch: 0 [17280/20000 (86%)]\tLoss: 4.513541\n",
            "Update Epoch: 0 [17600/20000 (88%)]\tLoss: 5.041379\n",
            "Update Epoch: 0 [17920/20000 (90%)]\tLoss: 4.404012\n",
            "Update Epoch: 0 [18240/20000 (91%)]\tLoss: 4.523292\n",
            "Update Epoch: 0 [18560/20000 (93%)]\tLoss: 4.532695\n",
            "Update Epoch: 0 [18880/20000 (94%)]\tLoss: 4.532662\n",
            "Update Epoch: 0 [19200/20000 (96%)]\tLoss: 4.459039\n",
            "Update Epoch: 0 [19520/20000 (98%)]\tLoss: 4.655740\n",
            "Update Epoch: 0 [19840/20000 (99%)]\tLoss: 4.759326\n",
            "Update Epoch: 1 [0/20000 (0%)]\tLoss: 4.729192\n",
            "Update Epoch: 1 [320/20000 (2%)]\tLoss: 4.270770\n",
            "Update Epoch: 1 [640/20000 (3%)]\tLoss: 4.714164\n",
            "Update Epoch: 1 [960/20000 (5%)]\tLoss: 4.982925\n",
            "Update Epoch: 1 [1280/20000 (6%)]\tLoss: 4.535493\n",
            "Update Epoch: 1 [1600/20000 (8%)]\tLoss: 5.025799\n",
            "Update Epoch: 1 [1920/20000 (10%)]\tLoss: 4.067478\n",
            "Update Epoch: 1 [2240/20000 (11%)]\tLoss: 4.324948\n",
            "Update Epoch: 1 [2560/20000 (13%)]\tLoss: 4.685992\n",
            "Update Epoch: 1 [2880/20000 (14%)]\tLoss: 4.499763\n",
            "Update Epoch: 1 [3200/20000 (16%)]\tLoss: 4.666074\n",
            "Update Epoch: 1 [3520/20000 (18%)]\tLoss: 4.293789\n",
            "Update Epoch: 1 [3840/20000 (19%)]\tLoss: 4.598932\n",
            "Update Epoch: 1 [4160/20000 (21%)]\tLoss: 4.306966\n",
            "Update Epoch: 1 [4480/20000 (22%)]\tLoss: 4.589346\n",
            "Update Epoch: 1 [4800/20000 (24%)]\tLoss: 4.446280\n",
            "Update Epoch: 1 [5120/20000 (26%)]\tLoss: 4.766521\n",
            "Update Epoch: 1 [5440/20000 (27%)]\tLoss: 4.647579\n",
            "Update Epoch: 1 [5760/20000 (29%)]\tLoss: 4.163697\n",
            "Update Epoch: 1 [6080/20000 (30%)]\tLoss: 4.498515\n",
            "Update Epoch: 1 [6400/20000 (32%)]\tLoss: 4.701214\n",
            "Update Epoch: 1 [6720/20000 (34%)]\tLoss: 4.187352\n",
            "Update Epoch: 1 [7040/20000 (35%)]\tLoss: 4.696900\n",
            "Update Epoch: 1 [7360/20000 (37%)]\tLoss: 4.380194\n",
            "Update Epoch: 1 [7680/20000 (38%)]\tLoss: 4.241879\n",
            "Update Epoch: 1 [8000/20000 (40%)]\tLoss: 3.898092\n",
            "Update Epoch: 1 [8320/20000 (42%)]\tLoss: 4.864954\n",
            "Update Epoch: 1 [8640/20000 (43%)]\tLoss: 3.906304\n",
            "Update Epoch: 1 [8960/20000 (45%)]\tLoss: 4.246606\n",
            "Update Epoch: 1 [9280/20000 (46%)]\tLoss: 4.371186\n",
            "Update Epoch: 1 [9600/20000 (48%)]\tLoss: 4.120362\n",
            "Update Epoch: 1 [9920/20000 (50%)]\tLoss: 3.861078\n",
            "Update Epoch: 1 [10240/20000 (51%)]\tLoss: 3.965154\n",
            "Update Epoch: 1 [10560/20000 (53%)]\tLoss: 4.231182\n",
            "Update Epoch: 1 [10880/20000 (54%)]\tLoss: 4.495487\n",
            "Update Epoch: 1 [11200/20000 (56%)]\tLoss: 4.537599\n",
            "Update Epoch: 1 [11520/20000 (58%)]\tLoss: 4.348183\n",
            "Update Epoch: 1 [11840/20000 (59%)]\tLoss: 4.223372\n",
            "Update Epoch: 1 [12160/20000 (61%)]\tLoss: 4.700541\n",
            "Update Epoch: 1 [12480/20000 (62%)]\tLoss: 4.591274\n",
            "Update Epoch: 1 [12800/20000 (64%)]\tLoss: 4.367082\n",
            "Update Epoch: 1 [13120/20000 (66%)]\tLoss: 3.974055\n",
            "Update Epoch: 1 [13440/20000 (67%)]\tLoss: 4.023993\n",
            "Update Epoch: 1 [13760/20000 (69%)]\tLoss: 3.446371\n",
            "Update Epoch: 1 [14080/20000 (70%)]\tLoss: 3.976901\n",
            "Update Epoch: 1 [14400/20000 (72%)]\tLoss: 4.019394\n",
            "Update Epoch: 1 [14720/20000 (74%)]\tLoss: 4.277947\n",
            "Update Epoch: 1 [15040/20000 (75%)]\tLoss: 4.028694\n",
            "Update Epoch: 1 [15360/20000 (77%)]\tLoss: 3.503516\n",
            "Update Epoch: 1 [15680/20000 (78%)]\tLoss: 3.961476\n",
            "Update Epoch: 1 [16000/20000 (80%)]\tLoss: 4.292965\n",
            "Update Epoch: 1 [16320/20000 (82%)]\tLoss: 4.245659\n",
            "Update Epoch: 1 [16640/20000 (83%)]\tLoss: 3.786770\n",
            "Update Epoch: 1 [16960/20000 (85%)]\tLoss: 4.053454\n",
            "Update Epoch: 1 [17280/20000 (86%)]\tLoss: 4.012940\n",
            "Update Epoch: 1 [17600/20000 (88%)]\tLoss: 4.188653\n",
            "Update Epoch: 1 [17920/20000 (90%)]\tLoss: 4.313838\n",
            "Update Epoch: 1 [18240/20000 (91%)]\tLoss: 4.223274\n",
            "Update Epoch: 1 [18560/20000 (93%)]\tLoss: 4.420124\n",
            "Update Epoch: 1 [18880/20000 (94%)]\tLoss: 3.497330\n",
            "Update Epoch: 1 [19200/20000 (96%)]\tLoss: 3.743725\n",
            "Update Epoch: 1 [19520/20000 (98%)]\tLoss: 4.051346\n",
            "Update Epoch: 1 [19840/20000 (99%)]\tLoss: 4.038751\n",
            "Update Epoch: 2 [0/20000 (0%)]\tLoss: 4.471127\n",
            "Update Epoch: 2 [320/20000 (2%)]\tLoss: 4.452261\n",
            "Update Epoch: 2 [640/20000 (3%)]\tLoss: 3.633801\n",
            "Update Epoch: 2 [960/20000 (5%)]\tLoss: 3.937426\n",
            "Update Epoch: 2 [1280/20000 (6%)]\tLoss: 3.836434\n",
            "Update Epoch: 2 [1600/20000 (8%)]\tLoss: 3.753151\n",
            "Update Epoch: 2 [1920/20000 (10%)]\tLoss: 3.729220\n",
            "Update Epoch: 2 [2240/20000 (11%)]\tLoss: 3.514199\n",
            "Update Epoch: 2 [2560/20000 (13%)]\tLoss: 3.981616\n",
            "Update Epoch: 2 [2880/20000 (14%)]\tLoss: 3.617640\n",
            "Update Epoch: 2 [3200/20000 (16%)]\tLoss: 3.597374\n",
            "Update Epoch: 2 [3520/20000 (18%)]\tLoss: 3.519289\n",
            "Update Epoch: 2 [3840/20000 (19%)]\tLoss: 3.888292\n",
            "Update Epoch: 2 [4160/20000 (21%)]\tLoss: 3.380046\n",
            "Update Epoch: 2 [4480/20000 (22%)]\tLoss: 3.827250\n",
            "Update Epoch: 2 [4800/20000 (24%)]\tLoss: 3.721054\n",
            "Update Epoch: 2 [5120/20000 (26%)]\tLoss: 3.696317\n",
            "Update Epoch: 2 [5440/20000 (27%)]\tLoss: 3.727992\n",
            "Update Epoch: 2 [5760/20000 (29%)]\tLoss: 3.598106\n",
            "Update Epoch: 2 [6080/20000 (30%)]\tLoss: 3.750509\n",
            "Update Epoch: 2 [6400/20000 (32%)]\tLoss: 3.758038\n",
            "Update Epoch: 2 [6720/20000 (34%)]\tLoss: 3.681502\n",
            "Update Epoch: 2 [7040/20000 (35%)]\tLoss: 3.625957\n",
            "Update Epoch: 2 [7360/20000 (37%)]\tLoss: 3.039432\n",
            "Update Epoch: 2 [7680/20000 (38%)]\tLoss: 3.967449\n",
            "Update Epoch: 2 [8000/20000 (40%)]\tLoss: 3.926215\n",
            "Update Epoch: 2 [8320/20000 (42%)]\tLoss: 4.181633\n",
            "Update Epoch: 2 [8640/20000 (43%)]\tLoss: 4.028404\n",
            "Update Epoch: 2 [8960/20000 (45%)]\tLoss: 3.700647\n",
            "Update Epoch: 2 [9280/20000 (46%)]\tLoss: 4.094258\n",
            "Update Epoch: 2 [9600/20000 (48%)]\tLoss: 2.884600\n",
            "Update Epoch: 2 [9920/20000 (50%)]\tLoss: 4.184409\n",
            "Update Epoch: 2 [10240/20000 (51%)]\tLoss: 3.878677\n",
            "Update Epoch: 2 [10560/20000 (53%)]\tLoss: 3.825204\n",
            "Update Epoch: 2 [10880/20000 (54%)]\tLoss: 3.693492\n",
            "Update Epoch: 2 [11200/20000 (56%)]\tLoss: 3.718107\n",
            "Update Epoch: 2 [11520/20000 (58%)]\tLoss: 3.579182\n",
            "Update Epoch: 2 [11840/20000 (59%)]\tLoss: 3.388526\n",
            "Update Epoch: 2 [12160/20000 (61%)]\tLoss: 3.181557\n",
            "Update Epoch: 2 [12480/20000 (62%)]\tLoss: 3.752598\n",
            "Update Epoch: 2 [12800/20000 (64%)]\tLoss: 3.283723\n",
            "Update Epoch: 2 [13120/20000 (66%)]\tLoss: 3.831322\n",
            "Update Epoch: 2 [13440/20000 (67%)]\tLoss: 3.569597\n",
            "Update Epoch: 2 [13760/20000 (69%)]\tLoss: 3.410601\n",
            "Update Epoch: 2 [14080/20000 (70%)]\tLoss: 3.297794\n",
            "Update Epoch: 2 [14400/20000 (72%)]\tLoss: 3.742467\n",
            "Update Epoch: 2 [14720/20000 (74%)]\tLoss: 3.400743\n",
            "Update Epoch: 2 [15040/20000 (75%)]\tLoss: 4.009645\n",
            "Update Epoch: 2 [15360/20000 (77%)]\tLoss: 3.653395\n",
            "Update Epoch: 2 [15680/20000 (78%)]\tLoss: 3.428472\n",
            "Update Epoch: 2 [16000/20000 (80%)]\tLoss: 3.547064\n",
            "Update Epoch: 2 [16320/20000 (82%)]\tLoss: 3.658403\n",
            "Update Epoch: 2 [16640/20000 (83%)]\tLoss: 4.045300\n",
            "Update Epoch: 2 [16960/20000 (85%)]\tLoss: 3.428232\n",
            "Update Epoch: 2 [17280/20000 (86%)]\tLoss: 3.814685\n",
            "Update Epoch: 2 [17600/20000 (88%)]\tLoss: 3.557206\n",
            "Update Epoch: 2 [17920/20000 (90%)]\tLoss: 3.382892\n",
            "Update Epoch: 2 [18240/20000 (91%)]\tLoss: 3.391670\n",
            "Update Epoch: 2 [18560/20000 (93%)]\tLoss: 3.177143\n",
            "Update Epoch: 2 [18880/20000 (94%)]\tLoss: 3.192767\n",
            "Update Epoch: 2 [19200/20000 (96%)]\tLoss: 2.684915\n",
            "Update Epoch: 2 [19520/20000 (98%)]\tLoss: 3.780190\n",
            "Update Epoch: 2 [19840/20000 (99%)]\tLoss: 3.524049\n",
            "Update Epoch: 3 [0/20000 (0%)]\tLoss: 3.378580\n",
            "Update Epoch: 3 [320/20000 (2%)]\tLoss: 3.720487\n",
            "Update Epoch: 3 [640/20000 (3%)]\tLoss: 3.343807\n",
            "Update Epoch: 3 [960/20000 (5%)]\tLoss: 3.597432\n",
            "Update Epoch: 3 [1280/20000 (6%)]\tLoss: 2.270752\n",
            "Update Epoch: 3 [1600/20000 (8%)]\tLoss: 3.045182\n",
            "Update Epoch: 3 [1920/20000 (10%)]\tLoss: 3.682129\n",
            "Update Epoch: 3 [2240/20000 (11%)]\tLoss: 3.336680\n",
            "Update Epoch: 3 [2560/20000 (13%)]\tLoss: 3.552015\n",
            "Update Epoch: 3 [2880/20000 (14%)]\tLoss: 3.571581\n",
            "Update Epoch: 3 [3200/20000 (16%)]\tLoss: 3.085707\n",
            "Update Epoch: 3 [3520/20000 (18%)]\tLoss: 3.410266\n",
            "Update Epoch: 3 [3840/20000 (19%)]\tLoss: 3.142362\n",
            "Update Epoch: 3 [4160/20000 (21%)]\tLoss: 3.382741\n",
            "Update Epoch: 3 [4480/20000 (22%)]\tLoss: 3.651640\n",
            "Update Epoch: 3 [4800/20000 (24%)]\tLoss: 2.950141\n",
            "Update Epoch: 3 [5120/20000 (26%)]\tLoss: 3.130178\n",
            "Update Epoch: 3 [5440/20000 (27%)]\tLoss: 2.960125\n",
            "Update Epoch: 3 [5760/20000 (29%)]\tLoss: 3.612969\n",
            "Update Epoch: 3 [6080/20000 (30%)]\tLoss: 3.259372\n",
            "Update Epoch: 3 [6400/20000 (32%)]\tLoss: 3.001423\n",
            "Update Epoch: 3 [6720/20000 (34%)]\tLoss: 3.128603\n",
            "Update Epoch: 3 [7040/20000 (35%)]\tLoss: 3.408697\n",
            "Update Epoch: 3 [7360/20000 (37%)]\tLoss: 3.492328\n",
            "Update Epoch: 3 [7680/20000 (38%)]\tLoss: 3.033734\n",
            "Update Epoch: 3 [8000/20000 (40%)]\tLoss: 3.522606\n",
            "Update Epoch: 3 [8320/20000 (42%)]\tLoss: 3.404398\n",
            "Update Epoch: 3 [8640/20000 (43%)]\tLoss: 3.167230\n",
            "Update Epoch: 3 [8960/20000 (45%)]\tLoss: 3.005900\n",
            "Update Epoch: 3 [9280/20000 (46%)]\tLoss: 3.340738\n",
            "Update Epoch: 3 [9600/20000 (48%)]\tLoss: 2.468262\n",
            "Update Epoch: 3 [9920/20000 (50%)]\tLoss: 2.861386\n",
            "Update Epoch: 3 [10240/20000 (51%)]\tLoss: 3.140049\n",
            "Update Epoch: 3 [10560/20000 (53%)]\tLoss: 3.489780\n",
            "Update Epoch: 3 [10880/20000 (54%)]\tLoss: 3.411474\n",
            "Update Epoch: 3 [11200/20000 (56%)]\tLoss: 3.693783\n",
            "Update Epoch: 3 [11520/20000 (58%)]\tLoss: 3.086987\n",
            "Update Epoch: 3 [11840/20000 (59%)]\tLoss: 3.253054\n",
            "Update Epoch: 3 [12160/20000 (61%)]\tLoss: 2.993007\n",
            "Update Epoch: 3 [12480/20000 (62%)]\tLoss: 2.720421\n",
            "Update Epoch: 3 [12800/20000 (64%)]\tLoss: 3.265292\n",
            "Update Epoch: 3 [13120/20000 (66%)]\tLoss: 2.799181\n",
            "Update Epoch: 3 [13440/20000 (67%)]\tLoss: 3.735652\n",
            "Update Epoch: 3 [13760/20000 (69%)]\tLoss: 3.022420\n",
            "Update Epoch: 3 [14080/20000 (70%)]\tLoss: 4.036535\n",
            "Update Epoch: 3 [14400/20000 (72%)]\tLoss: 3.765824\n",
            "Update Epoch: 3 [14720/20000 (74%)]\tLoss: 3.539304\n",
            "Update Epoch: 3 [15040/20000 (75%)]\tLoss: 3.356300\n",
            "Update Epoch: 3 [15360/20000 (77%)]\tLoss: 3.386895\n",
            "Update Epoch: 3 [15680/20000 (78%)]\tLoss: 3.088057\n",
            "Update Epoch: 3 [16000/20000 (80%)]\tLoss: 3.033882\n",
            "Update Epoch: 3 [16320/20000 (82%)]\tLoss: 3.478007\n",
            "Update Epoch: 3 [16640/20000 (83%)]\tLoss: 3.456073\n",
            "Update Epoch: 3 [16960/20000 (85%)]\tLoss: 3.138986\n",
            "Update Epoch: 3 [17280/20000 (86%)]\tLoss: 3.165552\n",
            "Update Epoch: 3 [17600/20000 (88%)]\tLoss: 2.500775\n",
            "Update Epoch: 3 [17920/20000 (90%)]\tLoss: 3.362574\n",
            "Update Epoch: 3 [18240/20000 (91%)]\tLoss: 3.563633\n",
            "Update Epoch: 3 [18560/20000 (93%)]\tLoss: 3.171924\n",
            "Update Epoch: 3 [18880/20000 (94%)]\tLoss: 3.220277\n",
            "Update Epoch: 3 [19200/20000 (96%)]\tLoss: 3.215085\n",
            "Update Epoch: 3 [19520/20000 (98%)]\tLoss: 3.524115\n",
            "Update Epoch: 3 [19840/20000 (99%)]\tLoss: 3.619007\n",
            "Update Epoch: 4 [0/20000 (0%)]\tLoss: 3.203794\n",
            "Update Epoch: 4 [320/20000 (2%)]\tLoss: 3.012945\n",
            "Update Epoch: 4 [640/20000 (3%)]\tLoss: 3.683422\n",
            "Update Epoch: 4 [960/20000 (5%)]\tLoss: 2.994308\n",
            "Update Epoch: 4 [1280/20000 (6%)]\tLoss: 2.762655\n",
            "Update Epoch: 4 [1600/20000 (8%)]\tLoss: 2.308130\n",
            "Update Epoch: 4 [1920/20000 (10%)]\tLoss: 3.096075\n",
            "Update Epoch: 4 [2240/20000 (11%)]\tLoss: 3.558338\n",
            "Update Epoch: 4 [2560/20000 (13%)]\tLoss: 3.003955\n",
            "Update Epoch: 4 [2880/20000 (14%)]\tLoss: 3.263888\n",
            "Update Epoch: 4 [3200/20000 (16%)]\tLoss: 2.749002\n",
            "Update Epoch: 4 [3520/20000 (18%)]\tLoss: 2.975096\n",
            "Update Epoch: 4 [3840/20000 (19%)]\tLoss: 3.252258\n",
            "Update Epoch: 4 [4160/20000 (21%)]\tLoss: 3.521300\n",
            "Update Epoch: 4 [4480/20000 (22%)]\tLoss: 3.078763\n",
            "Update Epoch: 4 [4800/20000 (24%)]\tLoss: 3.409610\n",
            "Update Epoch: 4 [5120/20000 (26%)]\tLoss: 3.256167\n",
            "Update Epoch: 4 [5440/20000 (27%)]\tLoss: 3.198037\n",
            "Update Epoch: 4 [5760/20000 (29%)]\tLoss: 2.791131\n",
            "Update Epoch: 4 [6080/20000 (30%)]\tLoss: 3.114750\n",
            "Update Epoch: 4 [6400/20000 (32%)]\tLoss: 3.424925\n",
            "Update Epoch: 4 [6720/20000 (34%)]\tLoss: 2.747535\n",
            "Update Epoch: 4 [7040/20000 (35%)]\tLoss: 3.111840\n",
            "Update Epoch: 4 [7360/20000 (37%)]\tLoss: 2.890399\n",
            "Update Epoch: 4 [7680/20000 (38%)]\tLoss: 2.435180\n",
            "Update Epoch: 4 [8000/20000 (40%)]\tLoss: 3.295549\n",
            "Update Epoch: 4 [8320/20000 (42%)]\tLoss: 2.400722\n",
            "Update Epoch: 4 [8640/20000 (43%)]\tLoss: 2.840440\n",
            "Update Epoch: 4 [8960/20000 (45%)]\tLoss: 3.070938\n",
            "Update Epoch: 4 [9280/20000 (46%)]\tLoss: 3.493429\n",
            "Update Epoch: 4 [9600/20000 (48%)]\tLoss: 2.718891\n",
            "Update Epoch: 4 [9920/20000 (50%)]\tLoss: 3.809076\n",
            "Update Epoch: 4 [10240/20000 (51%)]\tLoss: 2.910898\n",
            "Update Epoch: 4 [10560/20000 (53%)]\tLoss: 2.887199\n",
            "Update Epoch: 4 [10880/20000 (54%)]\tLoss: 3.198719\n",
            "Update Epoch: 4 [11200/20000 (56%)]\tLoss: 2.990264\n",
            "Update Epoch: 4 [11520/20000 (58%)]\tLoss: 3.195097\n",
            "Update Epoch: 4 [11840/20000 (59%)]\tLoss: 2.403270\n",
            "Update Epoch: 4 [12160/20000 (61%)]\tLoss: 2.996752\n",
            "Update Epoch: 4 [12480/20000 (62%)]\tLoss: 3.534806\n",
            "Update Epoch: 4 [12800/20000 (64%)]\tLoss: 3.420554\n",
            "Update Epoch: 4 [13120/20000 (66%)]\tLoss: 2.069581\n",
            "Update Epoch: 4 [13440/20000 (67%)]\tLoss: 2.888546\n",
            "Update Epoch: 4 [13760/20000 (69%)]\tLoss: 2.662184\n",
            "Update Epoch: 4 [14080/20000 (70%)]\tLoss: 3.378718\n",
            "Update Epoch: 4 [14400/20000 (72%)]\tLoss: 3.367635\n",
            "Update Epoch: 4 [14720/20000 (74%)]\tLoss: 2.892148\n",
            "Update Epoch: 4 [15040/20000 (75%)]\tLoss: 2.451818\n",
            "Update Epoch: 4 [15360/20000 (77%)]\tLoss: 2.256876\n",
            "Update Epoch: 4 [15680/20000 (78%)]\tLoss: 3.126670\n",
            "Update Epoch: 4 [16000/20000 (80%)]\tLoss: 2.777135\n",
            "Update Epoch: 4 [16320/20000 (82%)]\tLoss: 2.492654\n",
            "Update Epoch: 4 [16640/20000 (83%)]\tLoss: 3.252986\n",
            "Update Epoch: 4 [16960/20000 (85%)]\tLoss: 2.545758\n",
            "Update Epoch: 4 [17280/20000 (86%)]\tLoss: 2.696202\n",
            "Update Epoch: 4 [17600/20000 (88%)]\tLoss: 3.059923\n",
            "Update Epoch: 4 [17920/20000 (90%)]\tLoss: 3.325728\n",
            "Update Epoch: 4 [18240/20000 (91%)]\tLoss: 3.124054\n",
            "Update Epoch: 4 [18560/20000 (93%)]\tLoss: 3.371249\n",
            "Update Epoch: 4 [18880/20000 (94%)]\tLoss: 3.038848\n",
            "Update Epoch: 4 [19200/20000 (96%)]\tLoss: 2.852266\n",
            "Update Epoch: 4 [19520/20000 (98%)]\tLoss: 2.680648\n",
            "Update Epoch: 4 [19840/20000 (99%)]\tLoss: 3.260087\n",
            "\n",
            "client 3\n",
            "\n",
            "Update Epoch: 0 [0/20000 (0%)]\tLoss: 10.542134\n",
            "Update Epoch: 0 [320/20000 (2%)]\tLoss: 8.214258\n",
            "Update Epoch: 0 [640/20000 (3%)]\tLoss: 6.740822\n",
            "Update Epoch: 0 [960/20000 (5%)]\tLoss: 6.738940\n",
            "Update Epoch: 0 [1280/20000 (6%)]\tLoss: 6.827105\n",
            "Update Epoch: 0 [1600/20000 (8%)]\tLoss: 6.300308\n",
            "Update Epoch: 0 [1920/20000 (10%)]\tLoss: 6.480357\n",
            "Update Epoch: 0 [2240/20000 (11%)]\tLoss: 6.123174\n",
            "Update Epoch: 0 [2560/20000 (13%)]\tLoss: 6.172274\n",
            "Update Epoch: 0 [2880/20000 (14%)]\tLoss: 6.054890\n",
            "Update Epoch: 0 [3200/20000 (16%)]\tLoss: 5.667780\n",
            "Update Epoch: 0 [3520/20000 (18%)]\tLoss: 5.719484\n",
            "Update Epoch: 0 [3840/20000 (19%)]\tLoss: 5.734047\n",
            "Update Epoch: 0 [4160/20000 (21%)]\tLoss: 5.957496\n",
            "Update Epoch: 0 [4480/20000 (22%)]\tLoss: 5.822316\n",
            "Update Epoch: 0 [4800/20000 (24%)]\tLoss: 5.889442\n",
            "Update Epoch: 0 [5120/20000 (26%)]\tLoss: 5.504246\n",
            "Update Epoch: 0 [5440/20000 (27%)]\tLoss: 5.691559\n",
            "Update Epoch: 0 [5760/20000 (29%)]\tLoss: 5.628420\n",
            "Update Epoch: 0 [6080/20000 (30%)]\tLoss: 5.488746\n",
            "Update Epoch: 0 [6400/20000 (32%)]\tLoss: 5.491794\n",
            "Update Epoch: 0 [6720/20000 (34%)]\tLoss: 5.679843\n",
            "Update Epoch: 0 [7040/20000 (35%)]\tLoss: 5.706870\n",
            "Update Epoch: 0 [7360/20000 (37%)]\tLoss: 5.690729\n",
            "Update Epoch: 0 [7680/20000 (38%)]\tLoss: 5.228305\n",
            "Update Epoch: 0 [8000/20000 (40%)]\tLoss: 5.372627\n",
            "Update Epoch: 0 [8320/20000 (42%)]\tLoss: 5.249918\n",
            "Update Epoch: 0 [8640/20000 (43%)]\tLoss: 5.123323\n",
            "Update Epoch: 0 [8960/20000 (45%)]\tLoss: 5.211919\n",
            "Update Epoch: 0 [9280/20000 (46%)]\tLoss: 5.532619\n",
            "Update Epoch: 0 [9600/20000 (48%)]\tLoss: 5.441038\n",
            "Update Epoch: 0 [9920/20000 (50%)]\tLoss: 5.243944\n",
            "Update Epoch: 0 [10240/20000 (51%)]\tLoss: 5.293528\n",
            "Update Epoch: 0 [10560/20000 (53%)]\tLoss: 5.392310\n",
            "Update Epoch: 0 [10880/20000 (54%)]\tLoss: 5.477492\n",
            "Update Epoch: 0 [11200/20000 (56%)]\tLoss: 5.124277\n",
            "Update Epoch: 0 [11520/20000 (58%)]\tLoss: 5.046283\n",
            "Update Epoch: 0 [11840/20000 (59%)]\tLoss: 5.200089\n",
            "Update Epoch: 0 [12160/20000 (61%)]\tLoss: 4.799169\n",
            "Update Epoch: 0 [12480/20000 (62%)]\tLoss: 5.320227\n",
            "Update Epoch: 0 [12800/20000 (64%)]\tLoss: 5.024220\n",
            "Update Epoch: 0 [13120/20000 (66%)]\tLoss: 4.939421\n",
            "Update Epoch: 0 [13440/20000 (67%)]\tLoss: 4.984522\n",
            "Update Epoch: 0 [13760/20000 (69%)]\tLoss: 5.097265\n",
            "Update Epoch: 0 [14080/20000 (70%)]\tLoss: 5.119422\n",
            "Update Epoch: 0 [14400/20000 (72%)]\tLoss: 5.140472\n",
            "Update Epoch: 0 [14720/20000 (74%)]\tLoss: 4.902019\n",
            "Update Epoch: 0 [15040/20000 (75%)]\tLoss: 5.041608\n",
            "Update Epoch: 0 [15360/20000 (77%)]\tLoss: 4.773554\n",
            "Update Epoch: 0 [15680/20000 (78%)]\tLoss: 4.769803\n",
            "Update Epoch: 0 [16000/20000 (80%)]\tLoss: 5.026298\n",
            "Update Epoch: 0 [16320/20000 (82%)]\tLoss: 4.860088\n",
            "Update Epoch: 0 [16640/20000 (83%)]\tLoss: 5.467074\n",
            "Update Epoch: 0 [16960/20000 (85%)]\tLoss: 4.605898\n",
            "Update Epoch: 0 [17280/20000 (86%)]\tLoss: 4.586954\n",
            "Update Epoch: 0 [17600/20000 (88%)]\tLoss: 4.804264\n",
            "Update Epoch: 0 [17920/20000 (90%)]\tLoss: 4.745964\n",
            "Update Epoch: 0 [18240/20000 (91%)]\tLoss: 4.993519\n",
            "Update Epoch: 0 [18560/20000 (93%)]\tLoss: 4.448534\n",
            "Update Epoch: 0 [18880/20000 (94%)]\tLoss: 4.833604\n",
            "Update Epoch: 0 [19200/20000 (96%)]\tLoss: 5.051834\n",
            "Update Epoch: 0 [19520/20000 (98%)]\tLoss: 4.956702\n",
            "Update Epoch: 0 [19840/20000 (99%)]\tLoss: 5.033854\n",
            "Update Epoch: 1 [0/20000 (0%)]\tLoss: 4.557046\n",
            "Update Epoch: 1 [320/20000 (2%)]\tLoss: 4.414959\n",
            "Update Epoch: 1 [640/20000 (3%)]\tLoss: 4.735950\n",
            "Update Epoch: 1 [960/20000 (5%)]\tLoss: 4.771049\n",
            "Update Epoch: 1 [1280/20000 (6%)]\tLoss: 3.981482\n",
            "Update Epoch: 1 [1600/20000 (8%)]\tLoss: 4.162552\n",
            "Update Epoch: 1 [1920/20000 (10%)]\tLoss: 4.289050\n",
            "Update Epoch: 1 [2240/20000 (11%)]\tLoss: 4.269673\n",
            "Update Epoch: 1 [2560/20000 (13%)]\tLoss: 4.641892\n",
            "Update Epoch: 1 [2880/20000 (14%)]\tLoss: 4.227232\n",
            "Update Epoch: 1 [3200/20000 (16%)]\tLoss: 4.648219\n",
            "Update Epoch: 1 [3520/20000 (18%)]\tLoss: 4.042347\n",
            "Update Epoch: 1 [3840/20000 (19%)]\tLoss: 4.250035\n",
            "Update Epoch: 1 [4160/20000 (21%)]\tLoss: 4.517362\n",
            "Update Epoch: 1 [4480/20000 (22%)]\tLoss: 4.644977\n",
            "Update Epoch: 1 [4800/20000 (24%)]\tLoss: 4.134051\n",
            "Update Epoch: 1 [5120/20000 (26%)]\tLoss: 4.654930\n",
            "Update Epoch: 1 [5440/20000 (27%)]\tLoss: 4.878536\n",
            "Update Epoch: 1 [5760/20000 (29%)]\tLoss: 4.888780\n",
            "Update Epoch: 1 [6080/20000 (30%)]\tLoss: 4.396377\n",
            "Update Epoch: 1 [6400/20000 (32%)]\tLoss: 4.872959\n",
            "Update Epoch: 1 [6720/20000 (34%)]\tLoss: 4.188987\n",
            "Update Epoch: 1 [7040/20000 (35%)]\tLoss: 4.123015\n",
            "Update Epoch: 1 [7360/20000 (37%)]\tLoss: 4.602389\n",
            "Update Epoch: 1 [7680/20000 (38%)]\tLoss: 4.799581\n",
            "Update Epoch: 1 [8000/20000 (40%)]\tLoss: 4.101428\n",
            "Update Epoch: 1 [8320/20000 (42%)]\tLoss: 4.221431\n",
            "Update Epoch: 1 [8640/20000 (43%)]\tLoss: 4.286376\n",
            "Update Epoch: 1 [8960/20000 (45%)]\tLoss: 4.162552\n",
            "Update Epoch: 1 [9280/20000 (46%)]\tLoss: 4.057982\n",
            "Update Epoch: 1 [9600/20000 (48%)]\tLoss: 4.383048\n",
            "Update Epoch: 1 [9920/20000 (50%)]\tLoss: 4.193340\n",
            "Update Epoch: 1 [10240/20000 (51%)]\tLoss: 4.687799\n",
            "Update Epoch: 1 [10560/20000 (53%)]\tLoss: 4.526268\n",
            "Update Epoch: 1 [10880/20000 (54%)]\tLoss: 4.687133\n",
            "Update Epoch: 1 [11200/20000 (56%)]\tLoss: 4.157724\n",
            "Update Epoch: 1 [11520/20000 (58%)]\tLoss: 4.680837\n",
            "Update Epoch: 1 [11840/20000 (59%)]\tLoss: 3.949934\n",
            "Update Epoch: 1 [12160/20000 (61%)]\tLoss: 4.542228\n",
            "Update Epoch: 1 [12480/20000 (62%)]\tLoss: 4.065707\n",
            "Update Epoch: 1 [12800/20000 (64%)]\tLoss: 4.161476\n",
            "Update Epoch: 1 [13120/20000 (66%)]\tLoss: 4.233074\n",
            "Update Epoch: 1 [13440/20000 (67%)]\tLoss: 4.900636\n",
            "Update Epoch: 1 [13760/20000 (69%)]\tLoss: 4.147537\n",
            "Update Epoch: 1 [14080/20000 (70%)]\tLoss: 4.007515\n",
            "Update Epoch: 1 [14400/20000 (72%)]\tLoss: 4.383511\n",
            "Update Epoch: 1 [14720/20000 (74%)]\tLoss: 3.360013\n",
            "Update Epoch: 1 [15040/20000 (75%)]\tLoss: 4.478842\n",
            "Update Epoch: 1 [15360/20000 (77%)]\tLoss: 4.432262\n",
            "Update Epoch: 1 [15680/20000 (78%)]\tLoss: 4.152795\n",
            "Update Epoch: 1 [16000/20000 (80%)]\tLoss: 3.764781\n",
            "Update Epoch: 1 [16320/20000 (82%)]\tLoss: 4.336868\n",
            "Update Epoch: 1 [16640/20000 (83%)]\tLoss: 4.382090\n",
            "Update Epoch: 1 [16960/20000 (85%)]\tLoss: 3.939651\n",
            "Update Epoch: 1 [17280/20000 (86%)]\tLoss: 4.210970\n",
            "Update Epoch: 1 [17600/20000 (88%)]\tLoss: 4.213624\n",
            "Update Epoch: 1 [17920/20000 (90%)]\tLoss: 4.151501\n",
            "Update Epoch: 1 [18240/20000 (91%)]\tLoss: 3.955104\n",
            "Update Epoch: 1 [18560/20000 (93%)]\tLoss: 4.401294\n",
            "Update Epoch: 1 [18880/20000 (94%)]\tLoss: 4.528311\n",
            "Update Epoch: 1 [19200/20000 (96%)]\tLoss: 4.592438\n",
            "Update Epoch: 1 [19520/20000 (98%)]\tLoss: 4.395461\n",
            "Update Epoch: 1 [19840/20000 (99%)]\tLoss: 4.054987\n",
            "Update Epoch: 2 [0/20000 (0%)]\tLoss: 3.705577\n",
            "Update Epoch: 2 [320/20000 (2%)]\tLoss: 3.801384\n",
            "Update Epoch: 2 [640/20000 (3%)]\tLoss: 3.817904\n",
            "Update Epoch: 2 [960/20000 (5%)]\tLoss: 4.001194\n",
            "Update Epoch: 2 [1280/20000 (6%)]\tLoss: 4.054412\n",
            "Update Epoch: 2 [1600/20000 (8%)]\tLoss: 4.030607\n",
            "Update Epoch: 2 [1920/20000 (10%)]\tLoss: 3.768569\n",
            "Update Epoch: 2 [2240/20000 (11%)]\tLoss: 3.950286\n",
            "Update Epoch: 2 [2560/20000 (13%)]\tLoss: 3.750542\n",
            "Update Epoch: 2 [2880/20000 (14%)]\tLoss: 4.115816\n",
            "Update Epoch: 2 [3200/20000 (16%)]\tLoss: 3.999207\n",
            "Update Epoch: 2 [3520/20000 (18%)]\tLoss: 3.729736\n",
            "Update Epoch: 2 [3840/20000 (19%)]\tLoss: 4.015455\n",
            "Update Epoch: 2 [4160/20000 (21%)]\tLoss: 3.569333\n",
            "Update Epoch: 2 [4480/20000 (22%)]\tLoss: 3.570861\n",
            "Update Epoch: 2 [4800/20000 (24%)]\tLoss: 3.309621\n",
            "Update Epoch: 2 [5120/20000 (26%)]\tLoss: 3.621964\n",
            "Update Epoch: 2 [5440/20000 (27%)]\tLoss: 3.285494\n",
            "Update Epoch: 2 [5760/20000 (29%)]\tLoss: 3.463057\n",
            "Update Epoch: 2 [6080/20000 (30%)]\tLoss: 4.124746\n",
            "Update Epoch: 2 [6400/20000 (32%)]\tLoss: 3.678595\n",
            "Update Epoch: 2 [6720/20000 (34%)]\tLoss: 3.883598\n",
            "Update Epoch: 2 [7040/20000 (35%)]\tLoss: 4.219336\n",
            "Update Epoch: 2 [7360/20000 (37%)]\tLoss: 4.045334\n",
            "Update Epoch: 2 [7680/20000 (38%)]\tLoss: 4.269201\n",
            "Update Epoch: 2 [8000/20000 (40%)]\tLoss: 4.289746\n",
            "Update Epoch: 2 [8320/20000 (42%)]\tLoss: 3.795314\n",
            "Update Epoch: 2 [8640/20000 (43%)]\tLoss: 3.674994\n",
            "Update Epoch: 2 [8960/20000 (45%)]\tLoss: 3.059557\n",
            "Update Epoch: 2 [9280/20000 (46%)]\tLoss: 3.643316\n",
            "Update Epoch: 2 [9600/20000 (48%)]\tLoss: 3.824100\n",
            "Update Epoch: 2 [9920/20000 (50%)]\tLoss: 3.138041\n",
            "Update Epoch: 2 [10240/20000 (51%)]\tLoss: 3.922790\n",
            "Update Epoch: 2 [10560/20000 (53%)]\tLoss: 3.395638\n",
            "Update Epoch: 2 [10880/20000 (54%)]\tLoss: 4.344185\n",
            "Update Epoch: 2 [11200/20000 (56%)]\tLoss: 3.539275\n",
            "Update Epoch: 2 [11520/20000 (58%)]\tLoss: 3.994338\n",
            "Update Epoch: 2 [11840/20000 (59%)]\tLoss: 3.602411\n",
            "Update Epoch: 2 [12160/20000 (61%)]\tLoss: 3.853782\n",
            "Update Epoch: 2 [12480/20000 (62%)]\tLoss: 3.253913\n",
            "Update Epoch: 2 [12800/20000 (64%)]\tLoss: 3.542450\n",
            "Update Epoch: 2 [13120/20000 (66%)]\tLoss: 3.899844\n",
            "Update Epoch: 2 [13440/20000 (67%)]\tLoss: 3.111973\n",
            "Update Epoch: 2 [13760/20000 (69%)]\tLoss: 3.601793\n",
            "Update Epoch: 2 [14080/20000 (70%)]\tLoss: 3.551379\n",
            "Update Epoch: 2 [14400/20000 (72%)]\tLoss: 3.788866\n",
            "Update Epoch: 2 [14720/20000 (74%)]\tLoss: 3.878543\n",
            "Update Epoch: 2 [15040/20000 (75%)]\tLoss: 3.496625\n",
            "Update Epoch: 2 [15360/20000 (77%)]\tLoss: 3.533589\n",
            "Update Epoch: 2 [15680/20000 (78%)]\tLoss: 4.081953\n",
            "Update Epoch: 2 [16000/20000 (80%)]\tLoss: 3.436806\n",
            "Update Epoch: 2 [16320/20000 (82%)]\tLoss: 4.044913\n",
            "Update Epoch: 2 [16640/20000 (83%)]\tLoss: 3.749558\n",
            "Update Epoch: 2 [16960/20000 (85%)]\tLoss: 3.626219\n",
            "Update Epoch: 2 [17280/20000 (86%)]\tLoss: 3.502995\n",
            "Update Epoch: 2 [17600/20000 (88%)]\tLoss: 4.518163\n",
            "Update Epoch: 2 [17920/20000 (90%)]\tLoss: 4.367527\n",
            "Update Epoch: 2 [18240/20000 (91%)]\tLoss: 3.744358\n",
            "Update Epoch: 2 [18560/20000 (93%)]\tLoss: 3.852104\n",
            "Update Epoch: 2 [18880/20000 (94%)]\tLoss: 3.817012\n",
            "Update Epoch: 2 [19200/20000 (96%)]\tLoss: 4.258628\n",
            "Update Epoch: 2 [19520/20000 (98%)]\tLoss: 3.667977\n",
            "Update Epoch: 2 [19840/20000 (99%)]\tLoss: 3.410918\n",
            "Update Epoch: 3 [0/20000 (0%)]\tLoss: 3.367025\n",
            "Update Epoch: 3 [320/20000 (2%)]\tLoss: 3.656949\n",
            "Update Epoch: 3 [640/20000 (3%)]\tLoss: 3.548660\n",
            "Update Epoch: 3 [960/20000 (5%)]\tLoss: 3.059970\n",
            "Update Epoch: 3 [1280/20000 (6%)]\tLoss: 3.625278\n",
            "Update Epoch: 3 [1600/20000 (8%)]\tLoss: 2.672270\n",
            "Update Epoch: 3 [1920/20000 (10%)]\tLoss: 3.764058\n",
            "Update Epoch: 3 [2240/20000 (11%)]\tLoss: 3.346045\n",
            "Update Epoch: 3 [2560/20000 (13%)]\tLoss: 3.512556\n",
            "Update Epoch: 3 [2880/20000 (14%)]\tLoss: 3.194846\n",
            "Update Epoch: 3 [3200/20000 (16%)]\tLoss: 3.404374\n",
            "Update Epoch: 3 [3520/20000 (18%)]\tLoss: 3.434862\n",
            "Update Epoch: 3 [3840/20000 (19%)]\tLoss: 3.353315\n",
            "Update Epoch: 3 [4160/20000 (21%)]\tLoss: 3.178613\n",
            "Update Epoch: 3 [4480/20000 (22%)]\tLoss: 2.818208\n",
            "Update Epoch: 3 [4800/20000 (24%)]\tLoss: 3.046775\n",
            "Update Epoch: 3 [5120/20000 (26%)]\tLoss: 2.962127\n",
            "Update Epoch: 3 [5440/20000 (27%)]\tLoss: 3.239590\n",
            "Update Epoch: 3 [5760/20000 (29%)]\tLoss: 2.665890\n",
            "Update Epoch: 3 [6080/20000 (30%)]\tLoss: 3.324337\n",
            "Update Epoch: 3 [6400/20000 (32%)]\tLoss: 3.283134\n",
            "Update Epoch: 3 [6720/20000 (34%)]\tLoss: 3.007825\n",
            "Update Epoch: 3 [7040/20000 (35%)]\tLoss: 3.247092\n",
            "Update Epoch: 3 [7360/20000 (37%)]\tLoss: 3.780703\n",
            "Update Epoch: 3 [7680/20000 (38%)]\tLoss: 3.608679\n",
            "Update Epoch: 3 [8000/20000 (40%)]\tLoss: 3.559866\n",
            "Update Epoch: 3 [8320/20000 (42%)]\tLoss: 3.540192\n",
            "Update Epoch: 3 [8640/20000 (43%)]\tLoss: 3.751487\n",
            "Update Epoch: 3 [8960/20000 (45%)]\tLoss: 3.211485\n",
            "Update Epoch: 3 [9280/20000 (46%)]\tLoss: 3.423562\n",
            "Update Epoch: 3 [9600/20000 (48%)]\tLoss: 3.757999\n",
            "Update Epoch: 3 [9920/20000 (50%)]\tLoss: 3.480911\n",
            "Update Epoch: 3 [10240/20000 (51%)]\tLoss: 3.473629\n",
            "Update Epoch: 3 [10560/20000 (53%)]\tLoss: 3.370106\n",
            "Update Epoch: 3 [10880/20000 (54%)]\tLoss: 3.272817\n",
            "Update Epoch: 3 [11200/20000 (56%)]\tLoss: 3.626246\n",
            "Update Epoch: 3 [11520/20000 (58%)]\tLoss: 3.322253\n",
            "Update Epoch: 3 [11840/20000 (59%)]\tLoss: 3.412265\n",
            "Update Epoch: 3 [12160/20000 (61%)]\tLoss: 3.304296\n",
            "Update Epoch: 3 [12480/20000 (62%)]\tLoss: 2.647974\n",
            "Update Epoch: 3 [12800/20000 (64%)]\tLoss: 2.747234\n",
            "Update Epoch: 3 [13120/20000 (66%)]\tLoss: 3.925205\n",
            "Update Epoch: 3 [13440/20000 (67%)]\tLoss: 2.979790\n",
            "Update Epoch: 3 [13760/20000 (69%)]\tLoss: 2.918384\n",
            "Update Epoch: 3 [14080/20000 (70%)]\tLoss: 2.995663\n",
            "Update Epoch: 3 [14400/20000 (72%)]\tLoss: 3.455289\n",
            "Update Epoch: 3 [14720/20000 (74%)]\tLoss: 3.312322\n",
            "Update Epoch: 3 [15040/20000 (75%)]\tLoss: 3.332641\n",
            "Update Epoch: 3 [15360/20000 (77%)]\tLoss: 3.331277\n",
            "Update Epoch: 3 [15680/20000 (78%)]\tLoss: 2.849982\n",
            "Update Epoch: 3 [16000/20000 (80%)]\tLoss: 3.167924\n",
            "Update Epoch: 3 [16320/20000 (82%)]\tLoss: 3.067723\n",
            "Update Epoch: 3 [16640/20000 (83%)]\tLoss: 3.027727\n",
            "Update Epoch: 3 [16960/20000 (85%)]\tLoss: 3.939789\n",
            "Update Epoch: 3 [17280/20000 (86%)]\tLoss: 3.338281\n",
            "Update Epoch: 3 [17600/20000 (88%)]\tLoss: 3.646064\n",
            "Update Epoch: 3 [17920/20000 (90%)]\tLoss: 3.361199\n",
            "Update Epoch: 3 [18240/20000 (91%)]\tLoss: 2.948406\n",
            "Update Epoch: 3 [18560/20000 (93%)]\tLoss: 3.210966\n",
            "Update Epoch: 3 [18880/20000 (94%)]\tLoss: 2.840347\n",
            "Update Epoch: 3 [19200/20000 (96%)]\tLoss: 3.358568\n",
            "Update Epoch: 3 [19520/20000 (98%)]\tLoss: 2.949350\n",
            "Update Epoch: 3 [19840/20000 (99%)]\tLoss: 3.229376\n",
            "Update Epoch: 4 [0/20000 (0%)]\tLoss: 2.735238\n",
            "Update Epoch: 4 [320/20000 (2%)]\tLoss: 2.885146\n",
            "Update Epoch: 4 [640/20000 (3%)]\tLoss: 3.424871\n",
            "Update Epoch: 4 [960/20000 (5%)]\tLoss: 3.132721\n",
            "Update Epoch: 4 [1280/20000 (6%)]\tLoss: 2.644270\n",
            "Update Epoch: 4 [1600/20000 (8%)]\tLoss: 3.174963\n",
            "Update Epoch: 4 [1920/20000 (10%)]\tLoss: 3.853918\n",
            "Update Epoch: 4 [2240/20000 (11%)]\tLoss: 2.656457\n",
            "Update Epoch: 4 [2560/20000 (13%)]\tLoss: 2.684489\n",
            "Update Epoch: 4 [2880/20000 (14%)]\tLoss: 2.830855\n",
            "Update Epoch: 4 [3200/20000 (16%)]\tLoss: 3.088681\n",
            "Update Epoch: 4 [3520/20000 (18%)]\tLoss: 2.799961\n",
            "Update Epoch: 4 [3840/20000 (19%)]\tLoss: 3.849796\n",
            "Update Epoch: 4 [4160/20000 (21%)]\tLoss: 3.105524\n",
            "Update Epoch: 4 [4480/20000 (22%)]\tLoss: 2.618183\n",
            "Update Epoch: 4 [4800/20000 (24%)]\tLoss: 2.943004\n",
            "Update Epoch: 4 [5120/20000 (26%)]\tLoss: 3.480364\n",
            "Update Epoch: 4 [5440/20000 (27%)]\tLoss: 2.704404\n",
            "Update Epoch: 4 [5760/20000 (29%)]\tLoss: 3.769453\n",
            "Update Epoch: 4 [6080/20000 (30%)]\tLoss: 3.028695\n",
            "Update Epoch: 4 [6400/20000 (32%)]\tLoss: 2.950849\n",
            "Update Epoch: 4 [6720/20000 (34%)]\tLoss: 2.762660\n",
            "Update Epoch: 4 [7040/20000 (35%)]\tLoss: 2.949183\n",
            "Update Epoch: 4 [7360/20000 (37%)]\tLoss: 3.292031\n",
            "Update Epoch: 4 [7680/20000 (38%)]\tLoss: 2.270985\n",
            "Update Epoch: 4 [8000/20000 (40%)]\tLoss: 2.902377\n",
            "Update Epoch: 4 [8320/20000 (42%)]\tLoss: 3.747067\n",
            "Update Epoch: 4 [8640/20000 (43%)]\tLoss: 3.675889\n",
            "Update Epoch: 4 [8960/20000 (45%)]\tLoss: 2.857750\n",
            "Update Epoch: 4 [9280/20000 (46%)]\tLoss: 2.719531\n",
            "Update Epoch: 4 [9600/20000 (48%)]\tLoss: 3.643075\n",
            "Update Epoch: 4 [9920/20000 (50%)]\tLoss: 2.620550\n",
            "Update Epoch: 4 [10240/20000 (51%)]\tLoss: 2.791823\n",
            "Update Epoch: 4 [10560/20000 (53%)]\tLoss: 3.394264\n",
            "Update Epoch: 4 [10880/20000 (54%)]\tLoss: 3.030443\n",
            "Update Epoch: 4 [11200/20000 (56%)]\tLoss: 3.195369\n",
            "Update Epoch: 4 [11520/20000 (58%)]\tLoss: 3.558943\n",
            "Update Epoch: 4 [11840/20000 (59%)]\tLoss: 2.547286\n",
            "Update Epoch: 4 [12160/20000 (61%)]\tLoss: 3.124221\n",
            "Update Epoch: 4 [12480/20000 (62%)]\tLoss: 3.559039\n",
            "Update Epoch: 4 [12800/20000 (64%)]\tLoss: 2.631164\n",
            "Update Epoch: 4 [13120/20000 (66%)]\tLoss: 2.676396\n",
            "Update Epoch: 4 [13440/20000 (67%)]\tLoss: 3.342026\n",
            "Update Epoch: 4 [13760/20000 (69%)]\tLoss: 2.904359\n",
            "Update Epoch: 4 [14080/20000 (70%)]\tLoss: 2.847525\n",
            "Update Epoch: 4 [14400/20000 (72%)]\tLoss: 3.389053\n",
            "Update Epoch: 4 [14720/20000 (74%)]\tLoss: 3.103725\n",
            "Update Epoch: 4 [15040/20000 (75%)]\tLoss: 3.644960\n",
            "Update Epoch: 4 [15360/20000 (77%)]\tLoss: 2.641146\n",
            "Update Epoch: 4 [15680/20000 (78%)]\tLoss: 2.724454\n",
            "Update Epoch: 4 [16000/20000 (80%)]\tLoss: 2.659966\n",
            "Update Epoch: 4 [16320/20000 (82%)]\tLoss: 2.856024\n",
            "Update Epoch: 4 [16640/20000 (83%)]\tLoss: 2.397510\n",
            "Update Epoch: 4 [16960/20000 (85%)]\tLoss: 2.542232\n",
            "Update Epoch: 4 [17280/20000 (86%)]\tLoss: 2.894175\n",
            "Update Epoch: 4 [17600/20000 (88%)]\tLoss: 3.250315\n",
            "Update Epoch: 4 [17920/20000 (90%)]\tLoss: 2.673788\n",
            "Update Epoch: 4 [18240/20000 (91%)]\tLoss: 3.243485\n",
            "Update Epoch: 4 [18560/20000 (93%)]\tLoss: 2.739701\n",
            "Update Epoch: 4 [18880/20000 (94%)]\tLoss: 3.107576\n",
            "Update Epoch: 4 [19200/20000 (96%)]\tLoss: 2.759176\n",
            "Update Epoch: 4 [19520/20000 (98%)]\tLoss: 2.872527\n",
            "Update Epoch: 4 [19840/20000 (99%)]\tLoss: 3.580038\n",
            "==============================\n",
            "Round   0, Train loss 3.940\n",
            "\n",
            "Test set: Average loss: 4.1127 \n",
            "Accuracy: 1535/10000 (15.35%)\n",
            "\n",
            "==============================\n",
            "\n",
            "Global epoch 1\n",
            "\n",
            "\n",
            "client 2\n",
            "\n",
            "Update Epoch: 0 [0/20000 (0%)]\tLoss: 3.183819\n",
            "Update Epoch: 0 [320/20000 (2%)]\tLoss: 3.501085\n",
            "Update Epoch: 0 [640/20000 (3%)]\tLoss: 2.856788\n",
            "Update Epoch: 0 [960/20000 (5%)]\tLoss: 2.838413\n",
            "Update Epoch: 0 [1280/20000 (6%)]\tLoss: 3.152785\n",
            "Update Epoch: 0 [1600/20000 (8%)]\tLoss: 3.662610\n",
            "Update Epoch: 0 [1920/20000 (10%)]\tLoss: 3.218831\n",
            "Update Epoch: 0 [2240/20000 (11%)]\tLoss: 3.154083\n",
            "Update Epoch: 0 [2560/20000 (13%)]\tLoss: 3.094302\n",
            "Update Epoch: 0 [2880/20000 (14%)]\tLoss: 3.036912\n",
            "Update Epoch: 0 [3200/20000 (16%)]\tLoss: 2.791741\n",
            "Update Epoch: 0 [3520/20000 (18%)]\tLoss: 3.160319\n",
            "Update Epoch: 0 [3840/20000 (19%)]\tLoss: 2.970849\n",
            "Update Epoch: 0 [4160/20000 (21%)]\tLoss: 3.347154\n",
            "Update Epoch: 0 [4480/20000 (22%)]\tLoss: 3.652472\n",
            "Update Epoch: 0 [4800/20000 (24%)]\tLoss: 3.169484\n",
            "Update Epoch: 0 [5120/20000 (26%)]\tLoss: 2.979905\n",
            "Update Epoch: 0 [5440/20000 (27%)]\tLoss: 2.894577\n",
            "Update Epoch: 0 [5760/20000 (29%)]\tLoss: 3.410496\n",
            "Update Epoch: 0 [6080/20000 (30%)]\tLoss: 3.630180\n",
            "Update Epoch: 0 [6400/20000 (32%)]\tLoss: 2.853328\n",
            "Update Epoch: 0 [6720/20000 (34%)]\tLoss: 3.937251\n",
            "Update Epoch: 0 [7040/20000 (35%)]\tLoss: 3.135419\n",
            "Update Epoch: 0 [7360/20000 (37%)]\tLoss: 3.631798\n",
            "Update Epoch: 0 [7680/20000 (38%)]\tLoss: 3.097544\n",
            "Update Epoch: 0 [8000/20000 (40%)]\tLoss: 3.407157\n",
            "Update Epoch: 0 [8320/20000 (42%)]\tLoss: 3.432026\n",
            "Update Epoch: 0 [8640/20000 (43%)]\tLoss: 3.108868\n",
            "Update Epoch: 0 [8960/20000 (45%)]\tLoss: 2.989715\n",
            "Update Epoch: 0 [9280/20000 (46%)]\tLoss: 2.850101\n",
            "Update Epoch: 0 [9600/20000 (48%)]\tLoss: 3.428288\n",
            "Update Epoch: 0 [9920/20000 (50%)]\tLoss: 2.312410\n",
            "Update Epoch: 0 [10240/20000 (51%)]\tLoss: 3.351295\n",
            "Update Epoch: 0 [10560/20000 (53%)]\tLoss: 3.376093\n",
            "Update Epoch: 0 [10880/20000 (54%)]\tLoss: 3.410543\n",
            "Update Epoch: 0 [11200/20000 (56%)]\tLoss: 3.008571\n",
            "Update Epoch: 0 [11520/20000 (58%)]\tLoss: 2.952064\n",
            "Update Epoch: 0 [11840/20000 (59%)]\tLoss: 2.558575\n",
            "Update Epoch: 0 [12160/20000 (61%)]\tLoss: 2.746747\n",
            "Update Epoch: 0 [12480/20000 (62%)]\tLoss: 2.921853\n",
            "Update Epoch: 0 [12800/20000 (64%)]\tLoss: 3.359581\n",
            "Update Epoch: 0 [13120/20000 (66%)]\tLoss: 3.614966\n",
            "Update Epoch: 0 [13440/20000 (67%)]\tLoss: 2.751283\n",
            "Update Epoch: 0 [13760/20000 (69%)]\tLoss: 3.547045\n",
            "Update Epoch: 0 [14080/20000 (70%)]\tLoss: 3.770396\n",
            "Update Epoch: 0 [14400/20000 (72%)]\tLoss: 2.947702\n",
            "Update Epoch: 0 [14720/20000 (74%)]\tLoss: 3.368794\n",
            "Update Epoch: 0 [15040/20000 (75%)]\tLoss: 3.043772\n",
            "Update Epoch: 0 [15360/20000 (77%)]\tLoss: 3.290699\n",
            "Update Epoch: 0 [15680/20000 (78%)]\tLoss: 3.455041\n",
            "Update Epoch: 0 [16000/20000 (80%)]\tLoss: 3.170358\n",
            "Update Epoch: 0 [16320/20000 (82%)]\tLoss: 3.243586\n",
            "Update Epoch: 0 [16640/20000 (83%)]\tLoss: 2.788085\n",
            "Update Epoch: 0 [16960/20000 (85%)]\tLoss: 3.140593\n",
            "Update Epoch: 0 [17280/20000 (86%)]\tLoss: 2.263914\n",
            "Update Epoch: 0 [17600/20000 (88%)]\tLoss: 2.564935\n",
            "Update Epoch: 0 [17920/20000 (90%)]\tLoss: 3.025141\n",
            "Update Epoch: 0 [18240/20000 (91%)]\tLoss: 2.794154\n",
            "Update Epoch: 0 [18560/20000 (93%)]\tLoss: 3.556585\n",
            "Update Epoch: 0 [18880/20000 (94%)]\tLoss: 2.810205\n",
            "Update Epoch: 0 [19200/20000 (96%)]\tLoss: 3.279141\n",
            "Update Epoch: 0 [19520/20000 (98%)]\tLoss: 2.736888\n",
            "Update Epoch: 0 [19840/20000 (99%)]\tLoss: 3.344513\n",
            "Update Epoch: 1 [0/20000 (0%)]\tLoss: 2.677686\n",
            "Update Epoch: 1 [320/20000 (2%)]\tLoss: 3.095506\n",
            "Update Epoch: 1 [640/20000 (3%)]\tLoss: 3.191649\n",
            "Update Epoch: 1 [960/20000 (5%)]\tLoss: 2.993241\n",
            "Update Epoch: 1 [1280/20000 (6%)]\tLoss: 2.354258\n",
            "Update Epoch: 1 [1600/20000 (8%)]\tLoss: 2.799052\n",
            "Update Epoch: 1 [1920/20000 (10%)]\tLoss: 2.937484\n",
            "Update Epoch: 1 [2240/20000 (11%)]\tLoss: 3.184257\n",
            "Update Epoch: 1 [2560/20000 (13%)]\tLoss: 2.912270\n",
            "Update Epoch: 1 [2880/20000 (14%)]\tLoss: 3.729062\n",
            "Update Epoch: 1 [3200/20000 (16%)]\tLoss: 3.222596\n",
            "Update Epoch: 1 [3520/20000 (18%)]\tLoss: 2.629354\n",
            "Update Epoch: 1 [3840/20000 (19%)]\tLoss: 2.234935\n",
            "Update Epoch: 1 [4160/20000 (21%)]\tLoss: 3.328490\n",
            "Update Epoch: 1 [4480/20000 (22%)]\tLoss: 2.658285\n",
            "Update Epoch: 1 [4800/20000 (24%)]\tLoss: 2.507158\n",
            "Update Epoch: 1 [5120/20000 (26%)]\tLoss: 2.343799\n",
            "Update Epoch: 1 [5440/20000 (27%)]\tLoss: 2.625852\n",
            "Update Epoch: 1 [5760/20000 (29%)]\tLoss: 2.656476\n",
            "Update Epoch: 1 [6080/20000 (30%)]\tLoss: 3.340367\n",
            "Update Epoch: 1 [6400/20000 (32%)]\tLoss: 3.179697\n",
            "Update Epoch: 1 [6720/20000 (34%)]\tLoss: 2.681950\n",
            "Update Epoch: 1 [7040/20000 (35%)]\tLoss: 2.185204\n",
            "Update Epoch: 1 [7360/20000 (37%)]\tLoss: 2.917109\n",
            "Update Epoch: 1 [7680/20000 (38%)]\tLoss: 3.110597\n",
            "Update Epoch: 1 [8000/20000 (40%)]\tLoss: 2.397744\n",
            "Update Epoch: 1 [8320/20000 (42%)]\tLoss: 2.520902\n",
            "Update Epoch: 1 [8640/20000 (43%)]\tLoss: 2.565434\n",
            "Update Epoch: 1 [8960/20000 (45%)]\tLoss: 2.542634\n",
            "Update Epoch: 1 [9280/20000 (46%)]\tLoss: 1.928203\n",
            "Update Epoch: 1 [9600/20000 (48%)]\tLoss: 3.136171\n",
            "Update Epoch: 1 [9920/20000 (50%)]\tLoss: 2.784782\n",
            "Update Epoch: 1 [10240/20000 (51%)]\tLoss: 2.857370\n",
            "Update Epoch: 1 [10560/20000 (53%)]\tLoss: 2.511485\n",
            "Update Epoch: 1 [10880/20000 (54%)]\tLoss: 2.732231\n",
            "Update Epoch: 1 [11200/20000 (56%)]\tLoss: 2.530763\n",
            "Update Epoch: 1 [11520/20000 (58%)]\tLoss: 2.741556\n",
            "Update Epoch: 1 [11840/20000 (59%)]\tLoss: 2.640882\n",
            "Update Epoch: 1 [12160/20000 (61%)]\tLoss: 2.792737\n",
            "Update Epoch: 1 [12480/20000 (62%)]\tLoss: 2.809880\n",
            "Update Epoch: 1 [12800/20000 (64%)]\tLoss: 2.843348\n",
            "Update Epoch: 1 [13120/20000 (66%)]\tLoss: 2.674343\n",
            "Update Epoch: 1 [13440/20000 (67%)]\tLoss: 3.222510\n",
            "Update Epoch: 1 [13760/20000 (69%)]\tLoss: 2.645702\n",
            "Update Epoch: 1 [14080/20000 (70%)]\tLoss: 2.802617\n",
            "Update Epoch: 1 [14400/20000 (72%)]\tLoss: 2.221043\n",
            "Update Epoch: 1 [14720/20000 (74%)]\tLoss: 2.691847\n",
            "Update Epoch: 1 [15040/20000 (75%)]\tLoss: 2.426515\n",
            "Update Epoch: 1 [15360/20000 (77%)]\tLoss: 2.679815\n",
            "Update Epoch: 1 [15680/20000 (78%)]\tLoss: 2.523199\n",
            "Update Epoch: 1 [16000/20000 (80%)]\tLoss: 2.599688\n",
            "Update Epoch: 1 [16320/20000 (82%)]\tLoss: 2.859606\n",
            "Update Epoch: 1 [16640/20000 (83%)]\tLoss: 2.749416\n",
            "Update Epoch: 1 [16960/20000 (85%)]\tLoss: 2.667789\n",
            "Update Epoch: 1 [17280/20000 (86%)]\tLoss: 2.892919\n",
            "Update Epoch: 1 [17600/20000 (88%)]\tLoss: 2.655912\n",
            "Update Epoch: 1 [17920/20000 (90%)]\tLoss: 2.616413\n",
            "Update Epoch: 1 [18240/20000 (91%)]\tLoss: 2.903533\n",
            "Update Epoch: 1 [18560/20000 (93%)]\tLoss: 2.615757\n",
            "Update Epoch: 1 [18880/20000 (94%)]\tLoss: 2.649707\n",
            "Update Epoch: 1 [19200/20000 (96%)]\tLoss: 3.016173\n",
            "Update Epoch: 1 [19520/20000 (98%)]\tLoss: 2.890625\n",
            "Update Epoch: 1 [19840/20000 (99%)]\tLoss: 2.232558\n",
            "Update Epoch: 2 [0/20000 (0%)]\tLoss: 2.818336\n",
            "Update Epoch: 2 [320/20000 (2%)]\tLoss: 3.051759\n",
            "Update Epoch: 2 [640/20000 (3%)]\tLoss: 2.380989\n",
            "Update Epoch: 2 [960/20000 (5%)]\tLoss: 2.297647\n",
            "Update Epoch: 2 [1280/20000 (6%)]\tLoss: 2.774995\n",
            "Update Epoch: 2 [1600/20000 (8%)]\tLoss: 2.125222\n",
            "Update Epoch: 2 [1920/20000 (10%)]\tLoss: 2.392263\n",
            "Update Epoch: 2 [2240/20000 (11%)]\tLoss: 2.507856\n",
            "Update Epoch: 2 [2560/20000 (13%)]\tLoss: 1.748816\n",
            "Update Epoch: 2 [2880/20000 (14%)]\tLoss: 2.161730\n",
            "Update Epoch: 2 [3200/20000 (16%)]\tLoss: 2.746843\n",
            "Update Epoch: 2 [3520/20000 (18%)]\tLoss: 2.372155\n",
            "Update Epoch: 2 [3840/20000 (19%)]\tLoss: 2.590254\n",
            "Update Epoch: 2 [4160/20000 (21%)]\tLoss: 2.135965\n",
            "Update Epoch: 2 [4480/20000 (22%)]\tLoss: 2.505126\n",
            "Update Epoch: 2 [4800/20000 (24%)]\tLoss: 2.074006\n",
            "Update Epoch: 2 [5120/20000 (26%)]\tLoss: 2.624057\n",
            "Update Epoch: 2 [5440/20000 (27%)]\tLoss: 2.791179\n",
            "Update Epoch: 2 [5760/20000 (29%)]\tLoss: 2.272009\n",
            "Update Epoch: 2 [6080/20000 (30%)]\tLoss: 2.152722\n",
            "Update Epoch: 2 [6400/20000 (32%)]\tLoss: 2.672809\n",
            "Update Epoch: 2 [6720/20000 (34%)]\tLoss: 2.337809\n",
            "Update Epoch: 2 [7040/20000 (35%)]\tLoss: 2.530270\n",
            "Update Epoch: 2 [7360/20000 (37%)]\tLoss: 2.096967\n",
            "Update Epoch: 2 [7680/20000 (38%)]\tLoss: 2.942366\n",
            "Update Epoch: 2 [8000/20000 (40%)]\tLoss: 2.377772\n",
            "Update Epoch: 2 [8320/20000 (42%)]\tLoss: 2.736129\n",
            "Update Epoch: 2 [8640/20000 (43%)]\tLoss: 2.429976\n",
            "Update Epoch: 2 [8960/20000 (45%)]\tLoss: 2.596241\n",
            "Update Epoch: 2 [9280/20000 (46%)]\tLoss: 2.848335\n",
            "Update Epoch: 2 [9600/20000 (48%)]\tLoss: 2.659148\n",
            "Update Epoch: 2 [9920/20000 (50%)]\tLoss: 2.726547\n",
            "Update Epoch: 2 [10240/20000 (51%)]\tLoss: 2.407733\n",
            "Update Epoch: 2 [10560/20000 (53%)]\tLoss: 2.100047\n",
            "Update Epoch: 2 [10880/20000 (54%)]\tLoss: 2.769057\n",
            "Update Epoch: 2 [11200/20000 (56%)]\tLoss: 2.690167\n",
            "Update Epoch: 2 [11520/20000 (58%)]\tLoss: 2.332648\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-40-1dc048c0dbbd>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf'\\nclient {idx}\\n'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0mlocal\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mLocalUpdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdataset_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midxs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdict_users\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m         \u001b[0mw\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlocal\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcopy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdeepcopy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnet_glob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m         \u001b[0mw_locals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mw\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mloss_locals\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/content/models/Update.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, net)\u001b[0m\n\u001b[1;32m     45\u001b[0m                 \u001b[0mimages\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimages\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m                 \u001b[0mnet\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 47\u001b[0;31m                 \u001b[0mlog_probs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimages\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     48\u001b[0m                 \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlog_probs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m                 \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/models/mobilenetv3.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    218\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 220\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/models/mobilenetv3.py\u001b[0m in \u001b[0;36m_forward_impl\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    208\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    209\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_forward_impl\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 210\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeatures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    211\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    212\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mavgpool\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torchvision/models/mobilenetv3.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 111\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mblock\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    112\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muse_res_connect\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    113\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    215\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    216\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 217\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    218\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1499\u001b[0m                 \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_pre_hooks\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0m_global_backward_hooks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1500\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1501\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1502\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1503\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    462\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 463\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_conv_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    464\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    465\u001b[0m \u001b[0;32mclass\u001b[0m \u001b[0mConv3d\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_ConvNd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36m_conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    457\u001b[0m                             \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstride\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m                             _pair(0), self.dilation, self.groups)\n\u001b[0;32m--> 459\u001b[0;31m         return F.conv2d(input, weight, bias, self.stride,\n\u001b[0m\u001b[1;32m    460\u001b[0m                         self.padding, self.dilation, self.groups)\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "for iter in range(args.epochs):\n",
        "    print(f'\\nGlobal epoch {iter}\\n')\n",
        "    w_locals, loss_locals = [], []\n",
        "    m = max(int(args.frac * args.num_users), 1)\n",
        "    idxs_users = np.random.choice(range(args.num_users), m, replace=False)\n",
        "    for idx in idxs_users:\n",
        "        print(f'\\nclient {idx}\\n')\n",
        "        local = LocalUpdate(args=args, dataset=dataset_train, idxs=dict_users[idx])\n",
        "        w, loss = local.train(net=copy.deepcopy(net_glob).to(args.device))\n",
        "        w_locals.append(w)\n",
        "        loss_locals.append(loss)\n",
        "    # update global weights\n",
        "    w_glob = FedAvg(w_locals)\n",
        "\n",
        "    # copy weight to net_glob\n",
        "    net_glob.load_state_dict(w_glob)\n",
        "\n",
        "    # print loss\n",
        "    loss_avg = sum(loss_locals) / len(loss_locals)\n",
        "    print('==============================')\n",
        "    print('Round {:3d}, Train loss {:.3f}'.format(iter, loss_avg))\n",
        "    loss_train.append(loss_avg)\n",
        "    writer.add_scalar('train_loss', loss_avg, iter)\n",
        "    test_acc, test_loss = test_img(net_glob, dataset_test, args)\n",
        "    writer.add_scalar('test_loss', test_loss, iter)\n",
        "    writer.add_scalar('test_acc', test_acc, iter)\n",
        "    print('==============================')\n",
        "    save_info = {\n",
        "        \"model\": net_glob.state_dict(),\n",
        "        \"epoch\": iter\n",
        "    }\n",
        "    # save model weights\n",
        "    if (iter+1) % 500 == 0:\n",
        "        save_path = f'./save2/{TAG}_{iter+1}es' if args.debug else f'./save/{TAG}_{iter+1}es'\n",
        "        torch.save(save_info, save_path)\n",
        "    if iter > 100 and test_acc > test_best_acc:\n",
        "        test_best_acc = test_acc\n",
        "        save_path = f'./save2/{TAG}_bst' if args.debug else f'./save/{TAG}_bst'\n",
        "        torch.save(save_info, save_path)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt"
      ],
      "metadata": {
        "id": "2gHsGquy0_nJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# plot loss curve\n",
        "plt.figure()\n",
        "plt.plot(range(len(loss_train)), loss_train)\n",
        "plt.ylabel('train_loss')\n",
        "plt.savefig('./save/fed_{}_{}_{}_C{}_iid{}.png'.format(args.dataset, args.model, args.epochs, args.frac, args.iid))\n",
        "\n"
      ],
      "metadata": {
        "id": "KIRvixMxyTTj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# testing\n",
        "net_glob.eval()\n",
        "acc_train, loss_train = test_img(net_glob, dataset_train, args)\n",
        "acc_test, loss_test = test_img(net_glob, dataset_test, args)\n",
        "print(\"Training accuracy: {:.2f}\".format(acc_train))\n",
        "print(\"Testing accuracy: {:.2f}\".format(acc_test))\n",
        "writer.close()"
      ],
      "metadata": {
        "id": "eae1Xby8yVnM"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}